{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4be13af",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a04429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from src.dataset import Dataset\n",
    "from src.soft_dtw_torch import SoftDTWTorch, squared_euclidean_distances, jacobian_sq_euc\n",
    "from src.softdtw_barycenter import softdtw_barycenter\n",
    "import torch.nn as nn\n",
    "import torch, numpy as np, random\n",
    "from fastdtw import fastdtw\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=1234):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2fb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_60_40(X):\n",
    "    X = torch.tensor(X, dtype=torch.float32)  \n",
    "    \n",
    "    N = X.shape[0]\n",
    "    T = X.shape[1]\n",
    "    idx = int(0.6 * T)\n",
    "\n",
    "    X_in = X[:, :idx]      \n",
    "    X_out = X[:, idx:]     \n",
    "\n",
    "    return X_in, X_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971008d2",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd08950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UCR dataset: ECG200\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(\"ECG200\")\n",
    "X_train, y_train, X_test, y_test = ds.load_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37991925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module): # very simple model\n",
    "    def __init__(self, input_len, output_len, hidden_dim=64):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_len, hidden_dim)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        h = self.act(self.fc1(x))\n",
    "        out = self.fc2(h)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe96b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_in, Xtrain_out = split_60_40(X_train)\n",
    "Xtest_in, Xtest_out = split_60_40(X_test)\n",
    "\n",
    "Xtrain_in_flat  = Xtrain_in.squeeze(-1)\n",
    "Xtrain_out_flat = Xtrain_out.squeeze(-1)\n",
    "\n",
    "Xtest_in_flat  = Xtest_in.squeeze(-1)\n",
    "Xtest_out_flat = Xtest_out.squeeze(-1)\n",
    "\n",
    "T_in  = Xtrain_in_flat.shape[1]\n",
    "T_out = Xtrain_out_flat.shape[1]\n",
    "\n",
    "set_seed(42)\n",
    "model = SimpleMLP(input_len=T_in, output_len=T_out, hidden_dim=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c7bfa",
   "metadata": {},
   "source": [
    "# Random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Init\n",
    "def train_model(model,criterion, name_model,n_epochs=1000):\n",
    "    loss_prec = float('inf')\n",
    "    criterion = criterion\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(n_epochs):  \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred_flat = model(Xtrain_in_flat)      \n",
    "        loss = criterion(y_pred_flat, Xtrain_out_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss_prec - loss.item() < 1e-6:\n",
    "            print(f\"Converged at epoch {epoch+1}\")\n",
    "            torch.save(model.state_dict(), f\"{name_model}.pth\")\n",
    "            break\n",
    "\n",
    "        loss_prec = loss.item()\n",
    "        \n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6ab6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_softdtw_criterion(gamma):\n",
    "    sdtw = SoftDTWTorch(gamma=gamma)\n",
    "    \n",
    "    def criterion(y_pred_flat, y_true_flat):\n",
    "        y_pred = y_pred_flat.unsqueeze(-1) \n",
    "        y_true = y_true_flat.unsqueeze(-1)\n",
    "        return sdtw(y_pred, y_true).mean()\n",
    "\n",
    "    return criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3841f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dtw(model, X_in, X_out):\n",
    "    dtw_vals = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred_flat = model(X_in)\n",
    "\n",
    "        for i in range(len(X_in)):\n",
    "            pred = y_pred_flat[i].cpu().numpy()\n",
    "            true = X_out[i].cpu().numpy()\n",
    "\n",
    "            dist, _ = fastdtw(pred, true)\n",
    "            dtw_vals.append(dist)\n",
    "\n",
    "    return float(np.mean(dtw_vals))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dea426f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 0.424313\n",
      "Epoch 11/5000, Loss: 0.224199\n",
      "Epoch 21/5000, Loss: 0.173191\n",
      "Epoch 31/5000, Loss: 0.153992\n",
      "Epoch 41/5000, Loss: 0.142147\n",
      "Epoch 51/5000, Loss: 0.130656\n",
      "Epoch 61/5000, Loss: 0.120439\n",
      "Epoch 71/5000, Loss: 0.111548\n",
      "Epoch 81/5000, Loss: 0.104047\n",
      "Epoch 91/5000, Loss: 0.097827\n",
      "Epoch 101/5000, Loss: 0.092728\n",
      "Epoch 111/5000, Loss: 0.088522\n",
      "Epoch 121/5000, Loss: 0.084998\n",
      "Epoch 131/5000, Loss: 0.082017\n",
      "Epoch 141/5000, Loss: 0.079474\n",
      "Epoch 151/5000, Loss: 0.077280\n",
      "Epoch 161/5000, Loss: 0.075364\n",
      "Epoch 171/5000, Loss: 0.073667\n",
      "Epoch 181/5000, Loss: 0.072144\n",
      "Epoch 191/5000, Loss: 0.070760\n",
      "Epoch 201/5000, Loss: 0.069486\n",
      "Epoch 211/5000, Loss: 0.068302\n",
      "Epoch 221/5000, Loss: 0.067192\n",
      "Epoch 231/5000, Loss: 0.066142\n",
      "Epoch 241/5000, Loss: 0.065144\n",
      "Epoch 251/5000, Loss: 0.064190\n",
      "Epoch 261/5000, Loss: 0.063274\n",
      "Epoch 271/5000, Loss: 0.062394\n",
      "Epoch 281/5000, Loss: 0.061544\n",
      "Epoch 291/5000, Loss: 0.060724\n",
      "Epoch 301/5000, Loss: 0.059928\n",
      "Epoch 311/5000, Loss: 0.059157\n",
      "Epoch 321/5000, Loss: 0.058405\n",
      "Epoch 331/5000, Loss: 0.057672\n",
      "Epoch 341/5000, Loss: 0.056955\n",
      "Epoch 351/5000, Loss: 0.056253\n",
      "Epoch 361/5000, Loss: 0.055562\n",
      "Epoch 371/5000, Loss: 0.054882\n",
      "Epoch 381/5000, Loss: 0.054212\n",
      "Epoch 391/5000, Loss: 0.053550\n",
      "Epoch 401/5000, Loss: 0.052895\n",
      "Epoch 411/5000, Loss: 0.052248\n",
      "Epoch 421/5000, Loss: 0.051607\n",
      "Epoch 431/5000, Loss: 0.050972\n",
      "Epoch 441/5000, Loss: 0.050343\n",
      "Epoch 451/5000, Loss: 0.049719\n",
      "Epoch 461/5000, Loss: 0.049101\n",
      "Epoch 471/5000, Loss: 0.048489\n",
      "Epoch 481/5000, Loss: 0.047883\n",
      "Epoch 491/5000, Loss: 0.047283\n",
      "Epoch 501/5000, Loss: 0.046688\n",
      "Epoch 511/5000, Loss: 0.046099\n",
      "Epoch 521/5000, Loss: 0.045516\n",
      "Epoch 531/5000, Loss: 0.044939\n",
      "Epoch 541/5000, Loss: 0.044368\n",
      "Epoch 551/5000, Loss: 0.043802\n",
      "Epoch 561/5000, Loss: 0.043243\n",
      "Epoch 571/5000, Loss: 0.042689\n",
      "Epoch 581/5000, Loss: 0.042141\n",
      "Epoch 591/5000, Loss: 0.041599\n",
      "Epoch 601/5000, Loss: 0.041063\n",
      "Epoch 611/5000, Loss: 0.040532\n",
      "Epoch 621/5000, Loss: 0.040007\n",
      "Epoch 631/5000, Loss: 0.039487\n",
      "Epoch 641/5000, Loss: 0.038973\n",
      "Epoch 651/5000, Loss: 0.038464\n",
      "Epoch 661/5000, Loss: 0.037961\n",
      "Epoch 671/5000, Loss: 0.037463\n",
      "Epoch 681/5000, Loss: 0.036970\n",
      "Epoch 691/5000, Loss: 0.036482\n",
      "Epoch 701/5000, Loss: 0.036000\n",
      "Epoch 711/5000, Loss: 0.035523\n",
      "Epoch 721/5000, Loss: 0.035051\n",
      "Epoch 731/5000, Loss: 0.034584\n",
      "Epoch 741/5000, Loss: 0.034122\n",
      "Epoch 751/5000, Loss: 0.033666\n",
      "Epoch 761/5000, Loss: 0.033214\n",
      "Epoch 771/5000, Loss: 0.032768\n",
      "Epoch 781/5000, Loss: 0.032327\n",
      "Epoch 791/5000, Loss: 0.031891\n",
      "Epoch 801/5000, Loss: 0.031460\n",
      "Epoch 811/5000, Loss: 0.031034\n",
      "Epoch 821/5000, Loss: 0.030614\n",
      "Epoch 831/5000, Loss: 0.030199\n",
      "Epoch 841/5000, Loss: 0.029789\n",
      "Epoch 851/5000, Loss: 0.029385\n",
      "Epoch 861/5000, Loss: 0.028987\n",
      "Epoch 871/5000, Loss: 0.028594\n",
      "Epoch 881/5000, Loss: 0.028207\n",
      "Epoch 891/5000, Loss: 0.027826\n",
      "Epoch 901/5000, Loss: 0.027450\n",
      "Epoch 911/5000, Loss: 0.027080\n",
      "Epoch 921/5000, Loss: 0.026716\n",
      "Epoch 931/5000, Loss: 0.026357\n",
      "Epoch 941/5000, Loss: 0.026005\n",
      "Epoch 951/5000, Loss: 0.025657\n",
      "Epoch 961/5000, Loss: 0.025316\n",
      "Epoch 971/5000, Loss: 0.024979\n",
      "Epoch 981/5000, Loss: 0.024649\n",
      "Epoch 991/5000, Loss: 0.024323\n",
      "Epoch 1001/5000, Loss: 0.024003\n",
      "Epoch 1011/5000, Loss: 0.023688\n",
      "Epoch 1021/5000, Loss: 0.023379\n",
      "Epoch 1031/5000, Loss: 0.023074\n",
      "Epoch 1041/5000, Loss: 0.022774\n",
      "Epoch 1051/5000, Loss: 0.022479\n",
      "Epoch 1061/5000, Loss: 0.022189\n",
      "Epoch 1071/5000, Loss: 0.021904\n",
      "Epoch 1081/5000, Loss: 0.021624\n",
      "Epoch 1091/5000, Loss: 0.021348\n",
      "Epoch 1101/5000, Loss: 0.021076\n",
      "Epoch 1111/5000, Loss: 0.020809\n",
      "Epoch 1121/5000, Loss: 0.020546\n",
      "Epoch 1131/5000, Loss: 0.020288\n",
      "Epoch 1141/5000, Loss: 0.020034\n",
      "Epoch 1151/5000, Loss: 0.019784\n",
      "Epoch 1161/5000, Loss: 0.019538\n",
      "Epoch 1171/5000, Loss: 0.019296\n",
      "Epoch 1181/5000, Loss: 0.019059\n",
      "Epoch 1191/5000, Loss: 0.018825\n",
      "Epoch 1201/5000, Loss: 0.018595\n",
      "Epoch 1211/5000, Loss: 0.018370\n",
      "Epoch 1221/5000, Loss: 0.018148\n",
      "Epoch 1231/5000, Loss: 0.017929\n",
      "Epoch 1241/5000, Loss: 0.017715\n",
      "Epoch 1251/5000, Loss: 0.017504\n",
      "Epoch 1261/5000, Loss: 0.017297\n",
      "Epoch 1271/5000, Loss: 0.017094\n",
      "Epoch 1281/5000, Loss: 0.016894\n",
      "Epoch 1291/5000, Loss: 0.016697\n",
      "Epoch 1301/5000, Loss: 0.016504\n",
      "Epoch 1311/5000, Loss: 0.016314\n",
      "Epoch 1321/5000, Loss: 0.016128\n",
      "Epoch 1331/5000, Loss: 0.015945\n",
      "Epoch 1341/5000, Loss: 0.015766\n",
      "Epoch 1351/5000, Loss: 0.015589\n",
      "Epoch 1361/5000, Loss: 0.015416\n",
      "Epoch 1371/5000, Loss: 0.015245\n",
      "Epoch 1381/5000, Loss: 0.015078\n",
      "Epoch 1391/5000, Loss: 0.014913\n",
      "Epoch 1401/5000, Loss: 0.014752\n",
      "Epoch 1411/5000, Loss: 0.014593\n",
      "Epoch 1421/5000, Loss: 0.014437\n",
      "Epoch 1431/5000, Loss: 0.014284\n",
      "Epoch 1441/5000, Loss: 0.014134\n",
      "Epoch 1451/5000, Loss: 0.013986\n",
      "Epoch 1461/5000, Loss: 0.013841\n",
      "Epoch 1471/5000, Loss: 0.013698\n",
      "Epoch 1481/5000, Loss: 0.013558\n",
      "Epoch 1491/5000, Loss: 0.013420\n",
      "Epoch 1501/5000, Loss: 0.013284\n",
      "Epoch 1511/5000, Loss: 0.013151\n",
      "Epoch 1521/5000, Loss: 0.013020\n",
      "Epoch 1531/5000, Loss: 0.012891\n",
      "Epoch 1541/5000, Loss: 0.012764\n",
      "Epoch 1551/5000, Loss: 0.012640\n",
      "Epoch 1561/5000, Loss: 0.012517\n",
      "Epoch 1571/5000, Loss: 0.012397\n",
      "Epoch 1581/5000, Loss: 0.012278\n",
      "Epoch 1591/5000, Loss: 0.012162\n",
      "Epoch 1601/5000, Loss: 0.012047\n",
      "Epoch 1611/5000, Loss: 0.011934\n",
      "Epoch 1621/5000, Loss: 0.011823\n",
      "Epoch 1631/5000, Loss: 0.011714\n",
      "Epoch 1641/5000, Loss: 0.011606\n",
      "Epoch 1651/5000, Loss: 0.011501\n",
      "Epoch 1661/5000, Loss: 0.011396\n",
      "Epoch 1671/5000, Loss: 0.011294\n",
      "Epoch 1681/5000, Loss: 0.011193\n",
      "Epoch 1691/5000, Loss: 0.011094\n",
      "Epoch 1701/5000, Loss: 0.010996\n",
      "Epoch 1711/5000, Loss: 0.010899\n",
      "Epoch 1721/5000, Loss: 0.010804\n",
      "Epoch 1731/5000, Loss: 0.010711\n",
      "Epoch 1741/5000, Loss: 0.010619\n",
      "Epoch 1751/5000, Loss: 0.010528\n",
      "Epoch 1761/5000, Loss: 0.010438\n",
      "Epoch 1771/5000, Loss: 0.010350\n",
      "Epoch 1781/5000, Loss: 0.010263\n",
      "Epoch 1791/5000, Loss: 0.010178\n",
      "Epoch 1801/5000, Loss: 0.010094\n",
      "Epoch 1811/5000, Loss: 0.010010\n",
      "Epoch 1821/5000, Loss: 0.009928\n",
      "Epoch 1831/5000, Loss: 0.009848\n",
      "Epoch 1841/5000, Loss: 0.009768\n",
      "Epoch 1851/5000, Loss: 0.009689\n",
      "Epoch 1861/5000, Loss: 0.009612\n",
      "Epoch 1871/5000, Loss: 0.009535\n",
      "Epoch 1881/5000, Loss: 0.009460\n",
      "Epoch 1891/5000, Loss: 0.009385\n",
      "Epoch 1901/5000, Loss: 0.009312\n",
      "Epoch 1911/5000, Loss: 0.009239\n",
      "Epoch 1921/5000, Loss: 0.009168\n",
      "Epoch 1931/5000, Loss: 0.009097\n",
      "Epoch 1941/5000, Loss: 0.009027\n",
      "Epoch 1951/5000, Loss: 0.008958\n",
      "Epoch 1961/5000, Loss: 0.008890\n",
      "Epoch 1971/5000, Loss: 0.008823\n",
      "Epoch 1981/5000, Loss: 0.008757\n",
      "Epoch 1991/5000, Loss: 0.008691\n",
      "Epoch 2001/5000, Loss: 0.008627\n",
      "Epoch 2011/5000, Loss: 0.008563\n",
      "Epoch 2021/5000, Loss: 0.008499\n",
      "Epoch 2031/5000, Loss: 0.008437\n",
      "Epoch 2041/5000, Loss: 0.008375\n",
      "Epoch 2051/5000, Loss: 0.008314\n",
      "Epoch 2061/5000, Loss: 0.008254\n",
      "Epoch 2071/5000, Loss: 0.008194\n",
      "Epoch 2081/5000, Loss: 0.008135\n",
      "Epoch 2091/5000, Loss: 0.008077\n",
      "Epoch 2101/5000, Loss: 0.008019\n",
      "Epoch 2111/5000, Loss: 0.007962\n",
      "Epoch 2121/5000, Loss: 0.007905\n",
      "Epoch 2131/5000, Loss: 0.007849\n",
      "Epoch 2141/5000, Loss: 0.007794\n",
      "Epoch 2151/5000, Loss: 0.007739\n",
      "Epoch 2161/5000, Loss: 0.007685\n",
      "Epoch 2171/5000, Loss: 0.007631\n",
      "Epoch 2181/5000, Loss: 0.007578\n",
      "Epoch 2191/5000, Loss: 0.007525\n",
      "Epoch 2201/5000, Loss: 0.007473\n",
      "Epoch 2211/5000, Loss: 0.007421\n",
      "Epoch 2221/5000, Loss: 0.007370\n",
      "Epoch 2231/5000, Loss: 0.007319\n",
      "Epoch 2241/5000, Loss: 0.007269\n",
      "Epoch 2251/5000, Loss: 0.007219\n",
      "Epoch 2261/5000, Loss: 0.007169\n",
      "Epoch 2271/5000, Loss: 0.007120\n",
      "Epoch 2281/5000, Loss: 0.007072\n",
      "Epoch 2291/5000, Loss: 0.007023\n",
      "Epoch 2301/5000, Loss: 0.006976\n",
      "Epoch 2311/5000, Loss: 0.006928\n",
      "Epoch 2321/5000, Loss: 0.006881\n",
      "Epoch 2331/5000, Loss: 0.006835\n",
      "Epoch 2341/5000, Loss: 0.006788\n",
      "Epoch 2351/5000, Loss: 0.006742\n",
      "Epoch 2361/5000, Loss: 0.006697\n",
      "Epoch 2371/5000, Loss: 0.006652\n",
      "Epoch 2381/5000, Loss: 0.006607\n",
      "Epoch 2391/5000, Loss: 0.006562\n",
      "Epoch 2401/5000, Loss: 0.006518\n",
      "Epoch 2411/5000, Loss: 0.006474\n",
      "Epoch 2421/5000, Loss: 0.006431\n",
      "Epoch 2431/5000, Loss: 0.006388\n",
      "Epoch 2441/5000, Loss: 0.006345\n",
      "Epoch 2451/5000, Loss: 0.006302\n",
      "Epoch 2461/5000, Loss: 0.006260\n",
      "Epoch 2471/5000, Loss: 0.006218\n",
      "Epoch 2481/5000, Loss: 0.006176\n",
      "Epoch 2491/5000, Loss: 0.006135\n",
      "Epoch 2501/5000, Loss: 0.006094\n",
      "Epoch 2511/5000, Loss: 0.006053\n",
      "Epoch 2521/5000, Loss: 0.006012\n",
      "Epoch 2531/5000, Loss: 0.005972\n",
      "Epoch 2541/5000, Loss: 0.005932\n",
      "Epoch 2551/5000, Loss: 0.005893\n",
      "Epoch 2561/5000, Loss: 0.005853\n",
      "Epoch 2571/5000, Loss: 0.005814\n",
      "Epoch 2581/5000, Loss: 0.005775\n",
      "Epoch 2591/5000, Loss: 0.005736\n",
      "Epoch 2601/5000, Loss: 0.005698\n",
      "Epoch 2611/5000, Loss: 0.005660\n",
      "Epoch 2621/5000, Loss: 0.005622\n",
      "Epoch 2631/5000, Loss: 0.005585\n",
      "Epoch 2641/5000, Loss: 0.005548\n",
      "Epoch 2651/5000, Loss: 0.005511\n",
      "Epoch 2661/5000, Loss: 0.005474\n",
      "Epoch 2671/5000, Loss: 0.005437\n",
      "Epoch 2681/5000, Loss: 0.005401\n",
      "Epoch 2691/5000, Loss: 0.005365\n",
      "Epoch 2701/5000, Loss: 0.005329\n",
      "Epoch 2711/5000, Loss: 0.005294\n",
      "Epoch 2721/5000, Loss: 0.005259\n",
      "Epoch 2731/5000, Loss: 0.005224\n",
      "Epoch 2741/5000, Loss: 0.005189\n",
      "Epoch 2751/5000, Loss: 0.005155\n",
      "Epoch 2761/5000, Loss: 0.005120\n",
      "Epoch 2771/5000, Loss: 0.005086\n",
      "Epoch 2781/5000, Loss: 0.005053\n",
      "Epoch 2791/5000, Loss: 0.005019\n",
      "Epoch 2801/5000, Loss: 0.004986\n",
      "Epoch 2811/5000, Loss: 0.004954\n",
      "Epoch 2821/5000, Loss: 0.004920\n",
      "Epoch 2831/5000, Loss: 0.004888\n",
      "Epoch 2841/5000, Loss: 0.004856\n",
      "Epoch 2851/5000, Loss: 0.004824\n",
      "Epoch 2861/5000, Loss: 0.004792\n",
      "Epoch 2871/5000, Loss: 0.004761\n",
      "Epoch 2881/5000, Loss: 0.004729\n",
      "Epoch 2891/5000, Loss: 0.004698\n",
      "Epoch 2901/5000, Loss: 0.004667\n",
      "Epoch 2911/5000, Loss: 0.004637\n",
      "Epoch 2921/5000, Loss: 0.004606\n",
      "Epoch 2931/5000, Loss: 0.004576\n",
      "Epoch 2941/5000, Loss: 0.004546\n",
      "Epoch 2951/5000, Loss: 0.004516\n",
      "Epoch 2961/5000, Loss: 0.004487\n",
      "Epoch 2971/5000, Loss: 0.004457\n",
      "Epoch 2981/5000, Loss: 0.004429\n",
      "Epoch 2991/5000, Loss: 0.004400\n",
      "Epoch 3001/5000, Loss: 0.004371\n",
      "Epoch 3011/5000, Loss: 0.004342\n",
      "Epoch 3021/5000, Loss: 0.004314\n",
      "Epoch 3031/5000, Loss: 0.004286\n",
      "Epoch 3041/5000, Loss: 0.004258\n",
      "Epoch 3051/5000, Loss: 0.004231\n",
      "Epoch 3061/5000, Loss: 0.004203\n",
      "Epoch 3071/5000, Loss: 0.004176\n",
      "Epoch 3081/5000, Loss: 0.004149\n",
      "Epoch 3091/5000, Loss: 0.004122\n",
      "Epoch 3101/5000, Loss: 0.004095\n",
      "Epoch 3111/5000, Loss: 0.004069\n",
      "Epoch 3121/5000, Loss: 0.004043\n",
      "Epoch 3131/5000, Loss: 0.004016\n",
      "Epoch 3141/5000, Loss: 0.003990\n",
      "Epoch 3151/5000, Loss: 0.003965\n",
      "Epoch 3161/5000, Loss: 0.003940\n",
      "Epoch 3171/5000, Loss: 0.003914\n",
      "Epoch 3181/5000, Loss: 0.003889\n",
      "Epoch 3191/5000, Loss: 0.003864\n",
      "Epoch 3201/5000, Loss: 0.003839\n",
      "Epoch 3211/5000, Loss: 0.003814\n",
      "Epoch 3221/5000, Loss: 0.003790\n",
      "Epoch 3231/5000, Loss: 0.003766\n",
      "Epoch 3241/5000, Loss: 0.003742\n",
      "Epoch 3251/5000, Loss: 0.003718\n",
      "Epoch 3261/5000, Loss: 0.003694\n",
      "Epoch 3271/5000, Loss: 0.003670\n",
      "Epoch 3281/5000, Loss: 0.003647\n",
      "Epoch 3291/5000, Loss: 0.003624\n",
      "Epoch 3301/5000, Loss: 0.003601\n",
      "Epoch 3311/5000, Loss: 0.003578\n",
      "Epoch 3321/5000, Loss: 0.003555\n",
      "Epoch 3331/5000, Loss: 0.003532\n",
      "Epoch 3341/5000, Loss: 0.003510\n",
      "Epoch 3351/5000, Loss: 0.003492\n",
      "Epoch 3361/5000, Loss: 0.003467\n",
      "Epoch 3371/5000, Loss: 0.003444\n",
      "Epoch 3381/5000, Loss: 0.003422\n",
      "Epoch 3391/5000, Loss: 0.003400\n",
      "Epoch 3401/5000, Loss: 0.003379\n",
      "Epoch 3411/5000, Loss: 0.003357\n",
      "Epoch 3421/5000, Loss: 0.003336\n",
      "Epoch 3431/5000, Loss: 0.003315\n",
      "Epoch 3441/5000, Loss: 0.003294\n",
      "Epoch 3451/5000, Loss: 0.003274\n",
      "Epoch 3461/5000, Loss: 0.003253\n",
      "Epoch 3471/5000, Loss: 0.003233\n",
      "Epoch 3481/5000, Loss: 0.003213\n",
      "Epoch 3491/5000, Loss: 0.003192\n",
      "Epoch 3501/5000, Loss: 0.003172\n",
      "Epoch 3511/5000, Loss: 0.003153\n",
      "Epoch 3521/5000, Loss: 0.003133\n",
      "Epoch 3531/5000, Loss: 0.003113\n",
      "Converged at epoch 3538\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model,nn.MSELoss(),\"simple_mlp_ecg200\",n_epochs=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2527bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 0.426614\n",
      "Epoch 11/5000, Loss: 0.220988\n",
      "Epoch 21/5000, Loss: 0.161612\n",
      "Epoch 31/5000, Loss: 0.145491\n",
      "Epoch 41/5000, Loss: 0.133501\n",
      "Epoch 51/5000, Loss: 0.122526\n",
      "Epoch 61/5000, Loss: 0.113346\n",
      "Epoch 71/5000, Loss: 0.105521\n",
      "Epoch 81/5000, Loss: 0.098974\n",
      "Epoch 91/5000, Loss: 0.093675\n",
      "Epoch 101/5000, Loss: 0.089412\n",
      "Epoch 111/5000, Loss: 0.085950\n",
      "Epoch 121/5000, Loss: 0.083084\n",
      "Epoch 131/5000, Loss: 0.080659\n",
      "Epoch 141/5000, Loss: 0.078569\n",
      "Epoch 151/5000, Loss: 0.076738\n",
      "Epoch 161/5000, Loss: 0.075109\n",
      "Epoch 171/5000, Loss: 0.073639\n",
      "Epoch 181/5000, Loss: 0.072297\n",
      "Epoch 191/5000, Loss: 0.071057\n",
      "Epoch 201/5000, Loss: 0.069902\n",
      "Epoch 211/5000, Loss: 0.068815\n",
      "Epoch 221/5000, Loss: 0.067784\n",
      "Epoch 231/5000, Loss: 0.066801\n",
      "Epoch 241/5000, Loss: 0.065858\n",
      "Epoch 251/5000, Loss: 0.064947\n",
      "Epoch 261/5000, Loss: 0.064065\n",
      "Epoch 271/5000, Loss: 0.063206\n",
      "Epoch 281/5000, Loss: 0.062367\n",
      "Epoch 291/5000, Loss: 0.061546\n",
      "Epoch 301/5000, Loss: 0.060739\n",
      "Epoch 311/5000, Loss: 0.059945\n",
      "Epoch 321/5000, Loss: 0.059162\n",
      "Epoch 331/5000, Loss: 0.058389\n",
      "Epoch 341/5000, Loss: 0.057625\n",
      "Epoch 351/5000, Loss: 0.056868\n",
      "Epoch 361/5000, Loss: 0.056119\n",
      "Epoch 371/5000, Loss: 0.055375\n",
      "Epoch 381/5000, Loss: 0.054636\n",
      "Epoch 391/5000, Loss: 0.053901\n",
      "Epoch 401/5000, Loss: 0.053170\n",
      "Epoch 411/5000, Loss: 0.052441\n",
      "Epoch 421/5000, Loss: 0.051714\n",
      "Epoch 431/5000, Loss: 0.050989\n",
      "Epoch 441/5000, Loss: 0.050266\n",
      "Epoch 451/5000, Loss: 0.049544\n",
      "Epoch 461/5000, Loss: 0.048824\n",
      "Epoch 471/5000, Loss: 0.048105\n",
      "Epoch 481/5000, Loss: 0.047388\n",
      "Epoch 491/5000, Loss: 0.046673\n",
      "Epoch 501/5000, Loss: 0.045962\n",
      "Epoch 511/5000, Loss: 0.045253\n",
      "Epoch 521/5000, Loss: 0.044548\n",
      "Epoch 531/5000, Loss: 0.043847\n",
      "Epoch 541/5000, Loss: 0.043152\n",
      "Epoch 551/5000, Loss: 0.042463\n",
      "Epoch 561/5000, Loss: 0.041780\n",
      "Epoch 571/5000, Loss: 0.041106\n",
      "Epoch 581/5000, Loss: 0.040439\n",
      "Epoch 591/5000, Loss: 0.039782\n",
      "Epoch 601/5000, Loss: 0.039135\n",
      "Epoch 611/5000, Loss: 0.038498\n",
      "Epoch 621/5000, Loss: 0.037873\n",
      "Epoch 631/5000, Loss: 0.037258\n",
      "Epoch 641/5000, Loss: 0.036656\n",
      "Epoch 651/5000, Loss: 0.036065\n",
      "Epoch 661/5000, Loss: 0.035485\n",
      "Epoch 671/5000, Loss: 0.034918\n",
      "Epoch 681/5000, Loss: 0.034362\n",
      "Epoch 691/5000, Loss: 0.033818\n",
      "Epoch 701/5000, Loss: 0.033285\n",
      "Epoch 711/5000, Loss: 0.032763\n",
      "Epoch 721/5000, Loss: 0.032252\n",
      "Epoch 731/5000, Loss: 0.031751\n",
      "Epoch 741/5000, Loss: 0.031261\n",
      "Epoch 751/5000, Loss: 0.030781\n",
      "Epoch 761/5000, Loss: 0.030311\n",
      "Epoch 771/5000, Loss: 0.029850\n",
      "Epoch 781/5000, Loss: 0.029399\n",
      "Epoch 791/5000, Loss: 0.028957\n",
      "Epoch 801/5000, Loss: 0.028525\n",
      "Epoch 811/5000, Loss: 0.028101\n",
      "Epoch 821/5000, Loss: 0.027686\n",
      "Epoch 831/5000, Loss: 0.027279\n",
      "Epoch 841/5000, Loss: 0.026880\n",
      "Epoch 851/5000, Loss: 0.026490\n",
      "Epoch 861/5000, Loss: 0.026107\n",
      "Epoch 871/5000, Loss: 0.025732\n",
      "Epoch 881/5000, Loss: 0.025365\n",
      "Epoch 891/5000, Loss: 0.025005\n",
      "Epoch 901/5000, Loss: 0.024653\n",
      "Epoch 911/5000, Loss: 0.024308\n",
      "Epoch 921/5000, Loss: 0.023970\n",
      "Epoch 931/5000, Loss: 0.023638\n",
      "Epoch 941/5000, Loss: 0.023314\n",
      "Epoch 951/5000, Loss: 0.022996\n",
      "Epoch 961/5000, Loss: 0.022685\n",
      "Epoch 971/5000, Loss: 0.022380\n",
      "Epoch 981/5000, Loss: 0.022082\n",
      "Epoch 991/5000, Loss: 0.021790\n",
      "Epoch 1001/5000, Loss: 0.021504\n",
      "Epoch 1011/5000, Loss: 0.021224\n",
      "Epoch 1021/5000, Loss: 0.020950\n",
      "Epoch 1031/5000, Loss: 0.020682\n",
      "Epoch 1041/5000, Loss: 0.020419\n",
      "Epoch 1051/5000, Loss: 0.020162\n",
      "Epoch 1061/5000, Loss: 0.019910\n",
      "Epoch 1071/5000, Loss: 0.019663\n",
      "Epoch 1081/5000, Loss: 0.019421\n",
      "Epoch 1091/5000, Loss: 0.019185\n",
      "Epoch 1101/5000, Loss: 0.018953\n",
      "Epoch 1111/5000, Loss: 0.018726\n",
      "Epoch 1121/5000, Loss: 0.018504\n",
      "Epoch 1131/5000, Loss: 0.018286\n",
      "Epoch 1141/5000, Loss: 0.018072\n",
      "Epoch 1151/5000, Loss: 0.017863\n",
      "Epoch 1161/5000, Loss: 0.017659\n",
      "Epoch 1171/5000, Loss: 0.017458\n",
      "Epoch 1181/5000, Loss: 0.017261\n",
      "Epoch 1191/5000, Loss: 0.017068\n",
      "Epoch 1201/5000, Loss: 0.016879\n",
      "Epoch 1211/5000, Loss: 0.016694\n",
      "Epoch 1221/5000, Loss: 0.016512\n",
      "Epoch 1231/5000, Loss: 0.016334\n",
      "Epoch 1241/5000, Loss: 0.016159\n",
      "Epoch 1251/5000, Loss: 0.015987\n",
      "Epoch 1261/5000, Loss: 0.015818\n",
      "Epoch 1271/5000, Loss: 0.015653\n",
      "Epoch 1281/5000, Loss: 0.015490\n",
      "Epoch 1291/5000, Loss: 0.015331\n",
      "Epoch 1301/5000, Loss: 0.015174\n",
      "Epoch 1311/5000, Loss: 0.015020\n",
      "Epoch 1321/5000, Loss: 0.014869\n",
      "Epoch 1331/5000, Loss: 0.014721\n",
      "Epoch 1341/5000, Loss: 0.014575\n",
      "Epoch 1351/5000, Loss: 0.014431\n",
      "Epoch 1361/5000, Loss: 0.014290\n",
      "Epoch 1371/5000, Loss: 0.014151\n",
      "Epoch 1381/5000, Loss: 0.014015\n",
      "Epoch 1391/5000, Loss: 0.013881\n",
      "Epoch 1401/5000, Loss: 0.013749\n",
      "Epoch 1411/5000, Loss: 0.013619\n",
      "Epoch 1421/5000, Loss: 0.013491\n",
      "Epoch 1431/5000, Loss: 0.013365\n",
      "Epoch 1441/5000, Loss: 0.013242\n",
      "Epoch 1451/5000, Loss: 0.013120\n",
      "Epoch 1461/5000, Loss: 0.013000\n",
      "Epoch 1471/5000, Loss: 0.012881\n",
      "Epoch 1481/5000, Loss: 0.012765\n",
      "Epoch 1491/5000, Loss: 0.012650\n",
      "Epoch 1501/5000, Loss: 0.012537\n",
      "Epoch 1511/5000, Loss: 0.012425\n",
      "Epoch 1521/5000, Loss: 0.012316\n",
      "Epoch 1531/5000, Loss: 0.012207\n",
      "Epoch 1541/5000, Loss: 0.012100\n",
      "Epoch 1551/5000, Loss: 0.011995\n",
      "Epoch 1561/5000, Loss: 0.011891\n",
      "Epoch 1571/5000, Loss: 0.011788\n",
      "Epoch 1581/5000, Loss: 0.011687\n",
      "Epoch 1591/5000, Loss: 0.011587\n",
      "Epoch 1601/5000, Loss: 0.011488\n",
      "Epoch 1611/5000, Loss: 0.011390\n",
      "Epoch 1621/5000, Loss: 0.011294\n",
      "Epoch 1631/5000, Loss: 0.011199\n",
      "Epoch 1641/5000, Loss: 0.011105\n",
      "Epoch 1651/5000, Loss: 0.011012\n",
      "Epoch 1661/5000, Loss: 0.010921\n",
      "Epoch 1671/5000, Loss: 0.010830\n",
      "Epoch 1681/5000, Loss: 0.010740\n",
      "Epoch 1691/5000, Loss: 0.010652\n",
      "Epoch 1701/5000, Loss: 0.010564\n",
      "Epoch 1711/5000, Loss: 0.010478\n",
      "Epoch 1721/5000, Loss: 0.010392\n",
      "Epoch 1731/5000, Loss: 0.010308\n",
      "Epoch 1741/5000, Loss: 0.010224\n",
      "Epoch 1751/5000, Loss: 0.010141\n",
      "Epoch 1761/5000, Loss: 0.010060\n",
      "Epoch 1771/5000, Loss: 0.009979\n",
      "Epoch 1781/5000, Loss: 0.009899\n",
      "Epoch 1791/5000, Loss: 0.009820\n",
      "Epoch 1801/5000, Loss: 0.009741\n",
      "Epoch 1811/5000, Loss: 0.009664\n",
      "Epoch 1821/5000, Loss: 0.009587\n",
      "Epoch 1831/5000, Loss: 0.009511\n",
      "Epoch 1841/5000, Loss: 0.009436\n",
      "Epoch 1851/5000, Loss: 0.009362\n",
      "Epoch 1861/5000, Loss: 0.009288\n",
      "Epoch 1871/5000, Loss: 0.009215\n",
      "Epoch 1881/5000, Loss: 0.009143\n",
      "Epoch 1891/5000, Loss: 0.009072\n",
      "Epoch 1901/5000, Loss: 0.009002\n",
      "Epoch 1911/5000, Loss: 0.008932\n",
      "Epoch 1921/5000, Loss: 0.008863\n",
      "Epoch 1931/5000, Loss: 0.008794\n",
      "Epoch 1941/5000, Loss: 0.008726\n",
      "Epoch 1951/5000, Loss: 0.008659\n",
      "Epoch 1961/5000, Loss: 0.008593\n",
      "Epoch 1971/5000, Loss: 0.008527\n",
      "Epoch 1981/5000, Loss: 0.008462\n",
      "Epoch 1991/5000, Loss: 0.008398\n",
      "Epoch 2001/5000, Loss: 0.008334\n",
      "Epoch 2011/5000, Loss: 0.008271\n",
      "Epoch 2021/5000, Loss: 0.008208\n",
      "Epoch 2031/5000, Loss: 0.008146\n",
      "Epoch 2041/5000, Loss: 0.008085\n",
      "Epoch 2051/5000, Loss: 0.008025\n",
      "Epoch 2061/5000, Loss: 0.007964\n",
      "Epoch 2071/5000, Loss: 0.007905\n",
      "Epoch 2081/5000, Loss: 0.007846\n",
      "Epoch 2091/5000, Loss: 0.007788\n",
      "Epoch 2101/5000, Loss: 0.007730\n",
      "Epoch 2111/5000, Loss: 0.007673\n",
      "Epoch 2121/5000, Loss: 0.007616\n",
      "Epoch 2131/5000, Loss: 0.007560\n",
      "Epoch 2141/5000, Loss: 0.007505\n",
      "Epoch 2151/5000, Loss: 0.007450\n",
      "Epoch 2161/5000, Loss: 0.007396\n",
      "Epoch 2171/5000, Loss: 0.007342\n",
      "Epoch 2181/5000, Loss: 0.007288\n",
      "Epoch 2191/5000, Loss: 0.007235\n",
      "Epoch 2201/5000, Loss: 0.007183\n",
      "Epoch 2211/5000, Loss: 0.007131\n",
      "Epoch 2221/5000, Loss: 0.007080\n",
      "Epoch 2231/5000, Loss: 0.007029\n",
      "Epoch 2241/5000, Loss: 0.006979\n",
      "Epoch 2251/5000, Loss: 0.006929\n",
      "Epoch 2261/5000, Loss: 0.006879\n",
      "Epoch 2271/5000, Loss: 0.006830\n",
      "Epoch 2281/5000, Loss: 0.006782\n",
      "Epoch 2291/5000, Loss: 0.006734\n",
      "Epoch 2301/5000, Loss: 0.006686\n",
      "Epoch 2311/5000, Loss: 0.006639\n",
      "Epoch 2321/5000, Loss: 0.006592\n",
      "Epoch 2331/5000, Loss: 0.006546\n",
      "Epoch 2341/5000, Loss: 0.006500\n",
      "Epoch 2351/5000, Loss: 0.006454\n",
      "Epoch 2361/5000, Loss: 0.006409\n",
      "Epoch 2371/5000, Loss: 0.006364\n",
      "Epoch 2381/5000, Loss: 0.006320\n",
      "Epoch 2391/5000, Loss: 0.006276\n",
      "Epoch 2401/5000, Loss: 0.006233\n",
      "Epoch 2411/5000, Loss: 0.006190\n",
      "Epoch 2421/5000, Loss: 0.006147\n",
      "Epoch 2431/5000, Loss: 0.006104\n",
      "Epoch 2441/5000, Loss: 0.006062\n",
      "Epoch 2451/5000, Loss: 0.006021\n",
      "Epoch 2461/5000, Loss: 0.005979\n",
      "Epoch 2471/5000, Loss: 0.005938\n",
      "Epoch 2481/5000, Loss: 0.005898\n",
      "Epoch 2491/5000, Loss: 0.005857\n",
      "Epoch 2501/5000, Loss: 0.005817\n",
      "Epoch 2511/5000, Loss: 0.005778\n",
      "Epoch 2521/5000, Loss: 0.005738\n",
      "Epoch 2531/5000, Loss: 0.005699\n",
      "Epoch 2541/5000, Loss: 0.005661\n",
      "Epoch 2551/5000, Loss: 0.005623\n",
      "Epoch 2561/5000, Loss: 0.005584\n",
      "Epoch 2571/5000, Loss: 0.005546\n",
      "Epoch 2581/5000, Loss: 0.005509\n",
      "Epoch 2591/5000, Loss: 0.005472\n",
      "Epoch 2601/5000, Loss: 0.005435\n",
      "Epoch 2611/5000, Loss: 0.005399\n",
      "Epoch 2621/5000, Loss: 0.005362\n",
      "Epoch 2631/5000, Loss: 0.005326\n",
      "Epoch 2641/5000, Loss: 0.005291\n",
      "Epoch 2651/5000, Loss: 0.005255\n",
      "Epoch 2661/5000, Loss: 0.005220\n",
      "Epoch 2671/5000, Loss: 0.005185\n",
      "Epoch 2681/5000, Loss: 0.005151\n",
      "Epoch 2691/5000, Loss: 0.005116\n",
      "Epoch 2701/5000, Loss: 0.005082\n",
      "Epoch 2711/5000, Loss: 0.005048\n",
      "Epoch 2721/5000, Loss: 0.005015\n",
      "Epoch 2731/5000, Loss: 0.004982\n",
      "Epoch 2741/5000, Loss: 0.004949\n",
      "Epoch 2751/5000, Loss: 0.004916\n",
      "Epoch 2761/5000, Loss: 0.004883\n",
      "Epoch 2771/5000, Loss: 0.004851\n",
      "Epoch 2781/5000, Loss: 0.004819\n",
      "Epoch 2791/5000, Loss: 0.004787\n",
      "Epoch 2801/5000, Loss: 0.004756\n",
      "Epoch 2811/5000, Loss: 0.004724\n",
      "Epoch 2821/5000, Loss: 0.004693\n",
      "Epoch 2831/5000, Loss: 0.004662\n",
      "Epoch 2841/5000, Loss: 0.004631\n",
      "Epoch 2851/5000, Loss: 0.004601\n",
      "Epoch 2861/5000, Loss: 0.004571\n",
      "Epoch 2871/5000, Loss: 0.004541\n",
      "Epoch 2881/5000, Loss: 0.004511\n",
      "Epoch 2891/5000, Loss: 0.004481\n",
      "Epoch 2901/5000, Loss: 0.004452\n",
      "Epoch 2911/5000, Loss: 0.004423\n",
      "Epoch 2921/5000, Loss: 0.004394\n",
      "Epoch 2931/5000, Loss: 0.004365\n",
      "Epoch 2941/5000, Loss: 0.004337\n",
      "Epoch 2951/5000, Loss: 0.004309\n",
      "Epoch 2961/5000, Loss: 0.004280\n",
      "Epoch 2971/5000, Loss: 0.004252\n",
      "Epoch 2981/5000, Loss: 0.004224\n",
      "Epoch 2991/5000, Loss: 0.004197\n",
      "Epoch 3001/5000, Loss: 0.004169\n",
      "Epoch 3011/5000, Loss: 0.004142\n",
      "Epoch 3021/5000, Loss: 0.004115\n",
      "Epoch 3031/5000, Loss: 0.004088\n",
      "Epoch 3041/5000, Loss: 0.004062\n",
      "Epoch 3051/5000, Loss: 0.004035\n",
      "Epoch 3061/5000, Loss: 0.004009\n",
      "Epoch 3071/5000, Loss: 0.003983\n",
      "Epoch 3081/5000, Loss: 0.003957\n",
      "Epoch 3091/5000, Loss: 0.003931\n",
      "Epoch 3101/5000, Loss: 0.003906\n",
      "Epoch 3111/5000, Loss: 0.003881\n",
      "Epoch 3121/5000, Loss: 0.003855\n",
      "Epoch 3131/5000, Loss: 0.003830\n",
      "Epoch 3141/5000, Loss: 0.003806\n",
      "Epoch 3151/5000, Loss: 0.003782\n",
      "Epoch 3161/5000, Loss: 0.003757\n",
      "Epoch 3171/5000, Loss: 0.003732\n",
      "Epoch 3181/5000, Loss: 0.003708\n",
      "Epoch 3191/5000, Loss: 0.003685\n",
      "Epoch 3201/5000, Loss: 0.003661\n",
      "Epoch 3211/5000, Loss: 0.003637\n",
      "Epoch 3221/5000, Loss: 0.003614\n",
      "Epoch 3231/5000, Loss: 0.003591\n",
      "Epoch 3241/5000, Loss: 0.003568\n",
      "Epoch 3251/5000, Loss: 0.003545\n",
      "Epoch 3261/5000, Loss: 0.003522\n",
      "Epoch 3271/5000, Loss: 0.003500\n",
      "Epoch 3281/5000, Loss: 0.003477\n",
      "Epoch 3291/5000, Loss: 0.003455\n",
      "Epoch 3301/5000, Loss: 0.003433\n",
      "Epoch 3311/5000, Loss: 0.003411\n",
      "Epoch 3321/5000, Loss: 0.003389\n",
      "Epoch 3331/5000, Loss: 0.003367\n",
      "Epoch 3341/5000, Loss: 0.003346\n",
      "Epoch 3351/5000, Loss: 0.003324\n",
      "Epoch 3361/5000, Loss: 0.003303\n",
      "Converged at epoch 3367\n",
      "\n",
      "=== SoftDTW(gamma=0.001) â€” Random init ===\n",
      "Epoch 1/5000, Loss: 1547.312256\n",
      "Epoch 11/5000, Loss: 842.227966\n",
      "Epoch 21/5000, Loss: 631.493286\n",
      "Epoch 31/5000, Loss: 554.472229\n",
      "Epoch 41/5000, Loss: 498.987915\n",
      "Epoch 51/5000, Loss: 452.288055\n",
      "Epoch 61/5000, Loss: 414.632233\n",
      "Epoch 71/5000, Loss: 384.992737\n",
      "Epoch 81/5000, Loss: 362.819672\n",
      "Epoch 91/5000, Loss: 346.378876\n",
      "Epoch 101/5000, Loss: 333.958160\n",
      "Epoch 111/5000, Loss: 324.021362\n",
      "Epoch 121/5000, Loss: 315.559357\n",
      "Epoch 131/5000, Loss: 308.095642\n",
      "Epoch 141/5000, Loss: 301.378052\n",
      "Epoch 151/5000, Loss: 295.247223\n",
      "Epoch 161/5000, Loss: 289.590057\n",
      "Epoch 171/5000, Loss: 284.319885\n",
      "Epoch 181/5000, Loss: 279.367004\n",
      "Epoch 191/5000, Loss: 274.674591\n",
      "Epoch 201/5000, Loss: 270.194946\n",
      "Epoch 211/5000, Loss: 265.888489\n",
      "Epoch 221/5000, Loss: 261.722931\n",
      "Epoch 231/5000, Loss: 257.673523\n",
      "Epoch 241/5000, Loss: 253.721558\n",
      "Epoch 251/5000, Loss: 249.853729\n",
      "Epoch 261/5000, Loss: 246.060547\n",
      "Epoch 271/5000, Loss: 242.334732\n",
      "Epoch 281/5000, Loss: 238.671097\n",
      "Epoch 291/5000, Loss: 235.065903\n",
      "Epoch 301/5000, Loss: 231.516312\n",
      "Epoch 311/5000, Loss: 228.020401\n",
      "Epoch 321/5000, Loss: 224.576279\n",
      "Epoch 331/5000, Loss: 221.182419\n",
      "Epoch 341/5000, Loss: 217.837036\n",
      "Epoch 351/5000, Loss: 214.538162\n",
      "Epoch 361/5000, Loss: 211.283508\n",
      "Epoch 371/5000, Loss: 208.070969\n",
      "Epoch 381/5000, Loss: 204.897949\n",
      "Epoch 391/5000, Loss: 201.762039\n",
      "Epoch 401/5000, Loss: 198.661148\n",
      "Epoch 411/5000, Loss: 195.593201\n",
      "Epoch 421/5000, Loss: 192.556549\n",
      "Epoch 431/5000, Loss: 189.549683\n",
      "Epoch 441/5000, Loss: 186.571777\n",
      "Epoch 451/5000, Loss: 183.622345\n",
      "Epoch 461/5000, Loss: 180.701523\n",
      "Epoch 471/5000, Loss: 177.810104\n",
      "Epoch 481/5000, Loss: 174.949478\n",
      "Epoch 491/5000, Loss: 172.121506\n",
      "Epoch 501/5000, Loss: 169.328247\n",
      "Epoch 511/5000, Loss: 166.572433\n",
      "Epoch 521/5000, Loss: 163.856552\n",
      "Epoch 531/5000, Loss: 161.182922\n",
      "Epoch 541/5000, Loss: 158.553680\n",
      "Epoch 551/5000, Loss: 155.970474\n",
      "Epoch 561/5000, Loss: 153.434814\n",
      "Epoch 571/5000, Loss: 150.947464\n",
      "Epoch 581/5000, Loss: 148.509094\n",
      "Epoch 591/5000, Loss: 146.119858\n",
      "Epoch 601/5000, Loss: 143.779755\n",
      "Epoch 611/5000, Loss: 141.488663\n",
      "Epoch 621/5000, Loss: 139.246033\n",
      "Epoch 631/5000, Loss: 137.051407\n",
      "Epoch 641/5000, Loss: 134.904175\n",
      "Epoch 651/5000, Loss: 132.803619\n",
      "Epoch 661/5000, Loss: 130.749008\n",
      "Epoch 671/5000, Loss: 128.739487\n",
      "Epoch 681/5000, Loss: 126.774216\n",
      "Epoch 691/5000, Loss: 124.852142\n",
      "Epoch 701/5000, Loss: 122.972275\n",
      "Epoch 711/5000, Loss: 121.133385\n",
      "Epoch 721/5000, Loss: 119.334351\n",
      "Epoch 731/5000, Loss: 117.573906\n",
      "Epoch 741/5000, Loss: 115.850784\n",
      "Epoch 751/5000, Loss: 114.163757\n",
      "Epoch 761/5000, Loss: 112.511612\n",
      "Epoch 771/5000, Loss: 110.893219\n",
      "Epoch 781/5000, Loss: 109.307587\n",
      "Epoch 791/5000, Loss: 107.753609\n",
      "Epoch 801/5000, Loss: 106.230461\n",
      "Epoch 811/5000, Loss: 104.737183\n",
      "Epoch 821/5000, Loss: 103.273064\n",
      "Epoch 831/5000, Loss: 101.837227\n",
      "Epoch 841/5000, Loss: 100.429016\n",
      "Epoch 851/5000, Loss: 99.047668\n",
      "Epoch 861/5000, Loss: 97.692535\n",
      "Epoch 871/5000, Loss: 96.362823\n",
      "Epoch 881/5000, Loss: 95.057877\n",
      "Epoch 891/5000, Loss: 93.776985\n",
      "Epoch 901/5000, Loss: 92.519531\n",
      "Epoch 911/5000, Loss: 91.284767\n",
      "Epoch 921/5000, Loss: 90.072128\n",
      "Epoch 931/5000, Loss: 88.880981\n",
      "Epoch 941/5000, Loss: 87.710716\n",
      "Epoch 951/5000, Loss: 86.560814\n",
      "Epoch 961/5000, Loss: 85.430756\n",
      "Epoch 971/5000, Loss: 84.320091\n",
      "Epoch 981/5000, Loss: 83.228409\n",
      "Epoch 991/5000, Loss: 82.155296\n",
      "Epoch 1001/5000, Loss: 81.100388\n",
      "Epoch 1011/5000, Loss: 80.063423\n",
      "Epoch 1021/5000, Loss: 79.044052\n",
      "Epoch 1031/5000, Loss: 78.042046\n",
      "Epoch 1041/5000, Loss: 77.057152\n",
      "Epoch 1051/5000, Loss: 76.089195\n",
      "Epoch 1061/5000, Loss: 75.137894\n",
      "Epoch 1071/5000, Loss: 74.203102\n",
      "Epoch 1081/5000, Loss: 73.284668\n",
      "Epoch 1091/5000, Loss: 72.382332\n",
      "Epoch 1101/5000, Loss: 71.495941\n",
      "Epoch 1111/5000, Loss: 70.625359\n",
      "Epoch 1121/5000, Loss: 69.770332\n",
      "Epoch 1131/5000, Loss: 68.930740\n",
      "Epoch 1141/5000, Loss: 68.106361\n",
      "Epoch 1151/5000, Loss: 67.297012\n",
      "Epoch 1161/5000, Loss: 66.502510\n",
      "Epoch 1171/5000, Loss: 65.722618\n",
      "Epoch 1181/5000, Loss: 64.957184\n",
      "Epoch 1191/5000, Loss: 64.205978\n",
      "Epoch 1201/5000, Loss: 63.468765\n",
      "Epoch 1211/5000, Loss: 62.745338\n",
      "Epoch 1221/5000, Loss: 62.035454\n",
      "Epoch 1231/5000, Loss: 61.338936\n",
      "Epoch 1241/5000, Loss: 60.655499\n",
      "Epoch 1251/5000, Loss: 59.984917\n",
      "Epoch 1261/5000, Loss: 59.326965\n",
      "Epoch 1271/5000, Loss: 58.681370\n",
      "Epoch 1281/5000, Loss: 58.047901\n",
      "Epoch 1291/5000, Loss: 57.426327\n",
      "Epoch 1301/5000, Loss: 56.816322\n",
      "Epoch 1311/5000, Loss: 56.217716\n",
      "Epoch 1321/5000, Loss: 55.630196\n",
      "Epoch 1331/5000, Loss: 55.053520\n",
      "Epoch 1341/5000, Loss: 54.487438\n",
      "Epoch 1351/5000, Loss: 53.931675\n",
      "Epoch 1361/5000, Loss: 53.385971\n",
      "Epoch 1371/5000, Loss: 52.850143\n",
      "Epoch 1381/5000, Loss: 52.323837\n",
      "Epoch 1391/5000, Loss: 51.806835\n",
      "Epoch 1401/5000, Loss: 51.298931\n",
      "Epoch 1411/5000, Loss: 50.799881\n",
      "Epoch 1421/5000, Loss: 50.309422\n",
      "Epoch 1431/5000, Loss: 49.827312\n",
      "Epoch 1441/5000, Loss: 49.353374\n",
      "Epoch 1451/5000, Loss: 48.887363\n",
      "Epoch 1461/5000, Loss: 48.429066\n",
      "Epoch 1471/5000, Loss: 47.978264\n",
      "Epoch 1481/5000, Loss: 47.534771\n",
      "Epoch 1491/5000, Loss: 47.098396\n",
      "Epoch 1501/5000, Loss: 46.668938\n",
      "Epoch 1511/5000, Loss: 46.246208\n",
      "Epoch 1521/5000, Loss: 45.830044\n",
      "Epoch 1531/5000, Loss: 45.420254\n",
      "Epoch 1541/5000, Loss: 45.016666\n",
      "Epoch 1551/5000, Loss: 44.619144\n",
      "Epoch 1561/5000, Loss: 44.227509\n",
      "Epoch 1571/5000, Loss: 43.841625\n",
      "Epoch 1581/5000, Loss: 43.461330\n",
      "Epoch 1591/5000, Loss: 43.086494\n",
      "Epoch 1601/5000, Loss: 42.716991\n",
      "Epoch 1611/5000, Loss: 42.352638\n",
      "Epoch 1621/5000, Loss: 41.993366\n",
      "Epoch 1631/5000, Loss: 41.639004\n",
      "Epoch 1641/5000, Loss: 41.289463\n",
      "Epoch 1651/5000, Loss: 40.944618\n",
      "Epoch 1661/5000, Loss: 40.604343\n",
      "Epoch 1671/5000, Loss: 40.268543\n",
      "Epoch 1681/5000, Loss: 39.937107\n",
      "Epoch 1691/5000, Loss: 39.609959\n",
      "Epoch 1701/5000, Loss: 39.286953\n",
      "Epoch 1711/5000, Loss: 38.968033\n",
      "Epoch 1721/5000, Loss: 38.653072\n",
      "Epoch 1731/5000, Loss: 38.342003\n",
      "Epoch 1741/5000, Loss: 38.034740\n",
      "Epoch 1751/5000, Loss: 37.731178\n",
      "Epoch 1761/5000, Loss: 37.431259\n",
      "Epoch 1771/5000, Loss: 37.134888\n",
      "Epoch 1781/5000, Loss: 36.841969\n",
      "Epoch 1791/5000, Loss: 36.552452\n",
      "Epoch 1801/5000, Loss: 36.266258\n",
      "Epoch 1811/5000, Loss: 35.983307\n",
      "Epoch 1821/5000, Loss: 35.703518\n",
      "Epoch 1831/5000, Loss: 35.426826\n",
      "Epoch 1841/5000, Loss: 35.153179\n",
      "Epoch 1851/5000, Loss: 34.882496\n",
      "Epoch 1861/5000, Loss: 34.614697\n",
      "Epoch 1871/5000, Loss: 34.349739\n",
      "Epoch 1881/5000, Loss: 34.087547\n",
      "Epoch 1891/5000, Loss: 33.828064\n",
      "Epoch 1901/5000, Loss: 33.571232\n",
      "Epoch 1911/5000, Loss: 33.316982\n",
      "Epoch 1921/5000, Loss: 33.065266\n",
      "Epoch 1931/5000, Loss: 32.815994\n",
      "Epoch 1941/5000, Loss: 32.569176\n",
      "Epoch 1951/5000, Loss: 32.324696\n",
      "Epoch 1961/5000, Loss: 32.082527\n",
      "Epoch 1971/5000, Loss: 31.842619\n",
      "Epoch 1981/5000, Loss: 31.604908\n",
      "Epoch 1991/5000, Loss: 31.369368\n",
      "Epoch 2001/5000, Loss: 31.135923\n",
      "Epoch 2011/5000, Loss: 30.904543\n",
      "Epoch 2021/5000, Loss: 30.675190\n",
      "Epoch 2031/5000, Loss: 30.447809\n",
      "Epoch 2041/5000, Loss: 30.222368\n",
      "Epoch 2051/5000, Loss: 29.998837\n",
      "Epoch 2061/5000, Loss: 29.777153\n",
      "Epoch 2071/5000, Loss: 29.557291\n",
      "Epoch 2081/5000, Loss: 29.339230\n",
      "Epoch 2091/5000, Loss: 29.122917\n",
      "Epoch 2101/5000, Loss: 28.908340\n",
      "Epoch 2111/5000, Loss: 28.695463\n",
      "Epoch 2121/5000, Loss: 28.484266\n",
      "Epoch 2131/5000, Loss: 28.274700\n",
      "Epoch 2141/5000, Loss: 28.066763\n",
      "Epoch 2151/5000, Loss: 27.860432\n",
      "Epoch 2161/5000, Loss: 27.655680\n",
      "Epoch 2171/5000, Loss: 27.452473\n",
      "Epoch 2181/5000, Loss: 27.250822\n",
      "Epoch 2191/5000, Loss: 27.050673\n",
      "Epoch 2201/5000, Loss: 26.852045\n",
      "Epoch 2211/5000, Loss: 26.654900\n",
      "Epoch 2221/5000, Loss: 26.459217\n",
      "Epoch 2231/5000, Loss: 26.265017\n",
      "Epoch 2241/5000, Loss: 26.072252\n",
      "Epoch 2251/5000, Loss: 25.880907\n",
      "Epoch 2261/5000, Loss: 25.690979\n",
      "Epoch 2271/5000, Loss: 25.502476\n",
      "Epoch 2281/5000, Loss: 25.315355\n",
      "Epoch 2291/5000, Loss: 25.129625\n",
      "Epoch 2301/5000, Loss: 24.945267\n",
      "Epoch 2311/5000, Loss: 24.762274\n",
      "Epoch 2321/5000, Loss: 24.580616\n",
      "Epoch 2331/5000, Loss: 24.400312\n",
      "Epoch 2341/5000, Loss: 24.221331\n",
      "Epoch 2351/5000, Loss: 24.043669\n",
      "Epoch 2361/5000, Loss: 23.867321\n",
      "Epoch 2371/5000, Loss: 23.692274\n",
      "Epoch 2381/5000, Loss: 23.518524\n",
      "Epoch 2391/5000, Loss: 23.346054\n",
      "Epoch 2401/5000, Loss: 23.174862\n",
      "Epoch 2411/5000, Loss: 23.004923\n",
      "Epoch 2421/5000, Loss: 22.836262\n",
      "Epoch 2431/5000, Loss: 22.668858\n",
      "Epoch 2441/5000, Loss: 22.502693\n",
      "Epoch 2451/5000, Loss: 22.337772\n",
      "Epoch 2461/5000, Loss: 22.174072\n",
      "Epoch 2471/5000, Loss: 22.011621\n",
      "Epoch 2481/5000, Loss: 21.850376\n",
      "Epoch 2491/5000, Loss: 21.690361\n",
      "Epoch 2501/5000, Loss: 21.531559\n",
      "Epoch 2511/5000, Loss: 21.373962\n",
      "Epoch 2521/5000, Loss: 21.217552\n",
      "Epoch 2531/5000, Loss: 21.062357\n",
      "Epoch 2541/5000, Loss: 20.908335\n",
      "Epoch 2551/5000, Loss: 20.755514\n",
      "Epoch 2561/5000, Loss: 20.603863\n",
      "Epoch 2571/5000, Loss: 20.453386\n",
      "Epoch 2581/5000, Loss: 20.304077\n",
      "Epoch 2591/5000, Loss: 20.155937\n",
      "Epoch 2601/5000, Loss: 20.008951\n",
      "Epoch 2611/5000, Loss: 19.863117\n",
      "Epoch 2621/5000, Loss: 19.718426\n",
      "Epoch 2631/5000, Loss: 19.574879\n",
      "Epoch 2641/5000, Loss: 19.432459\n",
      "Epoch 2651/5000, Loss: 19.291172\n",
      "Epoch 2661/5000, Loss: 19.151005\n",
      "Epoch 2671/5000, Loss: 19.011948\n",
      "Epoch 2681/5000, Loss: 18.873995\n",
      "Epoch 2691/5000, Loss: 18.737152\n",
      "Epoch 2701/5000, Loss: 18.601404\n",
      "Epoch 2711/5000, Loss: 18.466740\n",
      "Epoch 2721/5000, Loss: 18.333155\n",
      "Epoch 2731/5000, Loss: 18.200647\n",
      "Epoch 2741/5000, Loss: 18.069193\n",
      "Epoch 2751/5000, Loss: 17.938814\n",
      "Epoch 2761/5000, Loss: 17.809479\n",
      "Epoch 2771/5000, Loss: 17.681196\n",
      "Epoch 2781/5000, Loss: 17.553938\n",
      "Epoch 2791/5000, Loss: 17.427711\n",
      "Epoch 2801/5000, Loss: 17.302513\n",
      "Epoch 2811/5000, Loss: 17.178326\n",
      "Epoch 2821/5000, Loss: 17.055134\n",
      "Epoch 2831/5000, Loss: 16.932949\n",
      "Epoch 2841/5000, Loss: 16.811752\n",
      "Epoch 2851/5000, Loss: 16.691536\n",
      "Epoch 2861/5000, Loss: 16.572296\n",
      "Epoch 2871/5000, Loss: 16.454012\n",
      "Epoch 2881/5000, Loss: 16.336697\n",
      "Epoch 2891/5000, Loss: 16.220318\n",
      "Epoch 2901/5000, Loss: 16.104881\n",
      "Epoch 2911/5000, Loss: 15.990377\n",
      "Epoch 2921/5000, Loss: 15.876796\n",
      "Epoch 2931/5000, Loss: 15.764131\n",
      "Epoch 2941/5000, Loss: 15.652366\n",
      "Epoch 2951/5000, Loss: 15.541511\n",
      "Epoch 2961/5000, Loss: 15.431530\n",
      "Epoch 2971/5000, Loss: 15.322442\n",
      "Epoch 2981/5000, Loss: 15.214218\n",
      "Epoch 2991/5000, Loss: 15.106866\n",
      "Epoch 3001/5000, Loss: 15.000376\n",
      "Epoch 3011/5000, Loss: 14.894725\n",
      "Epoch 3021/5000, Loss: 14.789939\n",
      "Epoch 3031/5000, Loss: 14.686825\n",
      "Epoch 3041/5000, Loss: 14.583302\n",
      "Epoch 3051/5000, Loss: 14.480548\n",
      "Epoch 3061/5000, Loss: 14.379093\n",
      "Epoch 3071/5000, Loss: 14.278391\n",
      "Epoch 3081/5000, Loss: 14.178505\n",
      "Epoch 3091/5000, Loss: 14.079421\n",
      "Epoch 3101/5000, Loss: 13.981134\n",
      "Epoch 3111/5000, Loss: 13.883616\n",
      "Epoch 3121/5000, Loss: 13.786881\n",
      "Epoch 3131/5000, Loss: 13.690920\n",
      "Epoch 3141/5000, Loss: 13.595719\n",
      "Epoch 3151/5000, Loss: 13.501275\n",
      "Epoch 3161/5000, Loss: 13.407595\n",
      "Epoch 3171/5000, Loss: 13.314651\n",
      "Epoch 3181/5000, Loss: 13.222451\n",
      "Epoch 3191/5000, Loss: 13.130994\n",
      "Epoch 3201/5000, Loss: 13.040256\n",
      "Epoch 3211/5000, Loss: 12.950255\n",
      "Epoch 3221/5000, Loss: 12.860967\n",
      "Epoch 3231/5000, Loss: 12.772400\n",
      "Epoch 3241/5000, Loss: 12.684615\n",
      "Epoch 3251/5000, Loss: 12.598623\n",
      "Epoch 3261/5000, Loss: 12.511313\n",
      "Epoch 3271/5000, Loss: 12.425819\n",
      "Epoch 3281/5000, Loss: 12.340264\n",
      "Epoch 3291/5000, Loss: 12.255932\n",
      "Epoch 3301/5000, Loss: 12.172253\n",
      "Epoch 3311/5000, Loss: 12.089219\n",
      "Epoch 3321/5000, Loss: 12.006893\n",
      "Epoch 3331/5000, Loss: 11.925246\n",
      "Epoch 3341/5000, Loss: 11.844522\n",
      "Epoch 3351/5000, Loss: 11.765097\n",
      "Epoch 3361/5000, Loss: 11.684297\n",
      "Epoch 3371/5000, Loss: 11.605153\n",
      "Epoch 3381/5000, Loss: 11.526722\n",
      "Epoch 3391/5000, Loss: 11.448957\n",
      "Epoch 3401/5000, Loss: 11.371944\n",
      "Epoch 3411/5000, Loss: 11.297258\n",
      "Epoch 3421/5000, Loss: 11.222914\n",
      "Epoch 3431/5000, Loss: 11.144604\n",
      "Epoch 3441/5000, Loss: 11.069866\n",
      "Epoch 3451/5000, Loss: 10.995567\n",
      "Epoch 3461/5000, Loss: 10.922531\n",
      "Epoch 3471/5000, Loss: 10.852385\n",
      "Epoch 3481/5000, Loss: 10.784608\n",
      "Epoch 3491/5000, Loss: 10.707931\n",
      "Epoch 3501/5000, Loss: 10.635158\n",
      "Epoch 3511/5000, Loss: 10.564966\n",
      "Epoch 3521/5000, Loss: 10.495215\n",
      "Epoch 3531/5000, Loss: 10.426432\n",
      "Epoch 3541/5000, Loss: 10.362715\n",
      "Epoch 3551/5000, Loss: 10.295437\n",
      "Epoch 3561/5000, Loss: 10.222529\n",
      "Epoch 3571/5000, Loss: 10.155899\n",
      "Epoch 3581/5000, Loss: 10.089505\n",
      "Epoch 3591/5000, Loss: 10.024063\n",
      "Epoch 3601/5000, Loss: 9.959356\n",
      "Epoch 3611/5000, Loss: 9.899926\n",
      "Epoch 3621/5000, Loss: 9.835750\n",
      "Epoch 3631/5000, Loss: 9.769910\n",
      "Epoch 3641/5000, Loss: 9.705339\n",
      "Epoch 3651/5000, Loss: 9.642072\n",
      "Epoch 3661/5000, Loss: 9.579818\n",
      "Epoch 3671/5000, Loss: 9.518418\n",
      "Epoch 3681/5000, Loss: 9.457557\n",
      "Epoch 3691/5000, Loss: 9.403621\n",
      "Epoch 3701/5000, Loss: 9.340549\n",
      "Epoch 3711/5000, Loss: 9.277530\n",
      "Epoch 3721/5000, Loss: 9.218538\n",
      "Epoch 3731/5000, Loss: 9.160269\n",
      "Epoch 3741/5000, Loss: 9.102942\n",
      "Epoch 3751/5000, Loss: 9.054402\n",
      "Epoch 3761/5000, Loss: 8.987506\n",
      "Epoch 3771/5000, Loss: 8.931537\n",
      "Epoch 3781/5000, Loss: 8.875033\n",
      "Epoch 3791/5000, Loss: 8.818815\n",
      "Epoch 3801/5000, Loss: 8.763431\n",
      "Epoch 3811/5000, Loss: 8.708578\n",
      "Epoch 3821/5000, Loss: 8.658489\n",
      "Epoch 3831/5000, Loss: 8.614404\n",
      "Epoch 3841/5000, Loss: 8.551348\n",
      "Epoch 3851/5000, Loss: 8.494597\n",
      "Epoch 3861/5000, Loss: 8.440367\n",
      "Epoch 3871/5000, Loss: 8.387104\n",
      "Epoch 3881/5000, Loss: 8.334756\n",
      "Epoch 3891/5000, Loss: 8.283256\n",
      "Epoch 3901/5000, Loss: 8.242788\n",
      "Epoch 3911/5000, Loss: 8.182070\n",
      "Epoch 3921/5000, Loss: 8.130487\n",
      "Epoch 3931/5000, Loss: 8.079651\n",
      "Epoch 3941/5000, Loss: 8.029513\n",
      "Epoch 3951/5000, Loss: 7.980175\n",
      "Converged at epoch 3961\n",
      "\n",
      "--- RANDOM INITIALIZATION RESULTS ---\n",
      "euclid              : 6.3074\n",
      "softdtw_gamma0.001  : 6.3882\n"
     ]
    }
   ],
   "source": [
    "random_results = {}\n",
    "\n",
    "criterion_euclid = torch.nn.MSELoss()\n",
    "model = SimpleMLP(T_in, T_out, hidden_dim=64)\n",
    "model = train_model(model, criterion_euclid, \"rand_euclid\",n_epochs=5000)\n",
    "\n",
    "random_results[\"euclid\"] = evaluate_dtw(model, Xtest_in_flat, Xtest_out.squeeze(-1))\n",
    "\n",
    "\n",
    "gammas = [0.001]\n",
    "\n",
    "for gamma in gammas:\n",
    "    print(f\"\\n=== SoftDTW(gamma={gamma}) â€” Random init ===\")\n",
    "    \n",
    "    criterion = make_softdtw_criterion(gamma)\n",
    "    model = SimpleMLP(T_in, T_out, hidden_dim=64)   \n",
    "    model = train_model(model, criterion, f\"rand_softdtw_gamma{gamma}\",n_epochs=5000)\n",
    "\n",
    "    dtw_err = evaluate_dtw(model, Xtest_in_flat, Xtest_out.squeeze(-1))\n",
    "    random_results[f\"softdtw_gamma{gamma}\"] = dtw_err\n",
    "\n",
    "print(\"\\n--- RANDOM INITIALIZATION RESULTS ---\")\n",
    "for k, v in random_results.items():\n",
    "    print(f\"{k:20s}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e60eadd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'euclid': 6.307397786915826, 'softdtw_gamma0.001': 6.388235841140904}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc34fd",
   "metadata": {},
   "source": [
    "# Euclidean init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3fbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_euclidean_init_then_softdtw(gamma, n_epochs_euclid=3000, n_epochs_sdtw=3000):\n",
    "    print(f\"Euclidean initialization + SoftDTW(gamma={gamma})\")\n",
    "\n",
    "    model = SimpleMLP(T_in, T_out, hidden_dim=64)\n",
    "\n",
    "    print(\"Pretraining with Euclidean loss\")\n",
    "    criterion_euclid = torch.nn.MSELoss()\n",
    "    model = train_model(model, criterion_euclid, \n",
    "                        f\"euclid_init_stage1_gamma{gamma}\", \n",
    "                        n_epochs=n_epochs_euclid)\n",
    "\n",
    "    print(\"Fine-tuning with Soft-DTW\")\n",
    "    criterion_sdtw = make_softdtw_criterion(gamma)\n",
    "    model = train_model(model, criterion_sdtw, \n",
    "                        f\"euclid_init_stage2_gamma{gamma}\", \n",
    "                        n_epochs=n_epochs_sdtw)\n",
    "\n",
    "    dtw_err = evaluate_dtw(model, Xtest_in_flat, Xtest_out.squeeze(-1))\n",
    "    \n",
    "    return dtw_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ff61e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean initialization + SoftDTW(gamma=1.0)\n",
      "Pretraining with Euclidean loss\n",
      "Epoch 1/3000, Loss: 0.500295\n",
      "Epoch 11/3000, Loss: 0.239167\n",
      "Epoch 21/3000, Loss: 0.172820\n",
      "Epoch 31/3000, Loss: 0.154919\n",
      "Epoch 41/3000, Loss: 0.143262\n",
      "Epoch 51/3000, Loss: 0.132220\n",
      "Epoch 61/3000, Loss: 0.122694\n",
      "Epoch 71/3000, Loss: 0.114790\n",
      "Epoch 81/3000, Loss: 0.107922\n",
      "Epoch 91/3000, Loss: 0.102087\n",
      "Epoch 101/3000, Loss: 0.097190\n",
      "Epoch 111/3000, Loss: 0.093069\n",
      "Epoch 121/3000, Loss: 0.089599\n",
      "Epoch 131/3000, Loss: 0.086672\n",
      "Epoch 141/3000, Loss: 0.084193\n",
      "Epoch 151/3000, Loss: 0.082078\n",
      "Epoch 161/3000, Loss: 0.080252\n",
      "Epoch 171/3000, Loss: 0.078650\n",
      "Epoch 181/3000, Loss: 0.077223\n",
      "Epoch 191/3000, Loss: 0.075931\n",
      "Epoch 201/3000, Loss: 0.074743\n",
      "Epoch 211/3000, Loss: 0.073638\n",
      "Epoch 221/3000, Loss: 0.072596\n",
      "Epoch 231/3000, Loss: 0.071606\n",
      "Epoch 241/3000, Loss: 0.070657\n",
      "Epoch 251/3000, Loss: 0.069739\n",
      "Epoch 261/3000, Loss: 0.068848\n",
      "Epoch 271/3000, Loss: 0.067977\n",
      "Epoch 281/3000, Loss: 0.067125\n",
      "Epoch 291/3000, Loss: 0.066287\n",
      "Epoch 301/3000, Loss: 0.065462\n",
      "Epoch 311/3000, Loss: 0.064649\n",
      "Epoch 321/3000, Loss: 0.063846\n",
      "Epoch 331/3000, Loss: 0.063054\n",
      "Epoch 341/3000, Loss: 0.062270\n",
      "Epoch 351/3000, Loss: 0.061495\n",
      "Epoch 361/3000, Loss: 0.060727\n",
      "Epoch 371/3000, Loss: 0.059967\n",
      "Epoch 381/3000, Loss: 0.059215\n",
      "Epoch 391/3000, Loss: 0.058469\n",
      "Epoch 401/3000, Loss: 0.057729\n",
      "Epoch 411/3000, Loss: 0.056997\n",
      "Epoch 421/3000, Loss: 0.056270\n",
      "Epoch 431/3000, Loss: 0.055551\n",
      "Epoch 441/3000, Loss: 0.054839\n",
      "Epoch 451/3000, Loss: 0.054133\n",
      "Epoch 461/3000, Loss: 0.053434\n",
      "Epoch 471/3000, Loss: 0.052743\n",
      "Epoch 481/3000, Loss: 0.052060\n",
      "Epoch 491/3000, Loss: 0.051384\n",
      "Epoch 501/3000, Loss: 0.050716\n",
      "Epoch 511/3000, Loss: 0.050055\n",
      "Epoch 521/3000, Loss: 0.049403\n",
      "Epoch 531/3000, Loss: 0.048759\n",
      "Epoch 541/3000, Loss: 0.048122\n",
      "Epoch 551/3000, Loss: 0.047494\n",
      "Epoch 561/3000, Loss: 0.046874\n",
      "Epoch 571/3000, Loss: 0.046262\n",
      "Epoch 581/3000, Loss: 0.045658\n",
      "Epoch 591/3000, Loss: 0.045061\n",
      "Epoch 601/3000, Loss: 0.044471\n",
      "Epoch 611/3000, Loss: 0.043889\n",
      "Epoch 621/3000, Loss: 0.043314\n",
      "Epoch 631/3000, Loss: 0.042745\n",
      "Epoch 641/3000, Loss: 0.042183\n",
      "Epoch 651/3000, Loss: 0.041627\n",
      "Epoch 661/3000, Loss: 0.041077\n",
      "Epoch 671/3000, Loss: 0.040534\n",
      "Epoch 681/3000, Loss: 0.039996\n",
      "Epoch 691/3000, Loss: 0.039464\n",
      "Epoch 701/3000, Loss: 0.038938\n",
      "Epoch 711/3000, Loss: 0.038418\n",
      "Epoch 721/3000, Loss: 0.037903\n",
      "Epoch 731/3000, Loss: 0.037395\n",
      "Epoch 741/3000, Loss: 0.036894\n",
      "Epoch 751/3000, Loss: 0.036399\n",
      "Epoch 761/3000, Loss: 0.035910\n",
      "Epoch 771/3000, Loss: 0.035428\n",
      "Epoch 781/3000, Loss: 0.034954\n",
      "Epoch 791/3000, Loss: 0.034486\n",
      "Epoch 801/3000, Loss: 0.034026\n",
      "Epoch 811/3000, Loss: 0.033572\n",
      "Epoch 821/3000, Loss: 0.033126\n",
      "Epoch 831/3000, Loss: 0.032688\n",
      "Epoch 841/3000, Loss: 0.032256\n",
      "Epoch 851/3000, Loss: 0.031832\n",
      "Epoch 861/3000, Loss: 0.031415\n",
      "Epoch 871/3000, Loss: 0.031005\n",
      "Epoch 881/3000, Loss: 0.030602\n",
      "Epoch 891/3000, Loss: 0.030206\n",
      "Epoch 901/3000, Loss: 0.029817\n",
      "Epoch 911/3000, Loss: 0.029435\n",
      "Epoch 921/3000, Loss: 0.029059\n",
      "Epoch 931/3000, Loss: 0.028689\n",
      "Epoch 941/3000, Loss: 0.028326\n",
      "Epoch 951/3000, Loss: 0.027969\n",
      "Epoch 961/3000, Loss: 0.027618\n",
      "Epoch 971/3000, Loss: 0.027273\n",
      "Epoch 981/3000, Loss: 0.026934\n",
      "Epoch 991/3000, Loss: 0.026601\n",
      "Epoch 1001/3000, Loss: 0.026274\n",
      "Epoch 1011/3000, Loss: 0.025951\n",
      "Epoch 1021/3000, Loss: 0.025635\n",
      "Epoch 1031/3000, Loss: 0.025324\n",
      "Epoch 1041/3000, Loss: 0.025018\n",
      "Epoch 1051/3000, Loss: 0.024717\n",
      "Epoch 1061/3000, Loss: 0.024421\n",
      "Epoch 1071/3000, Loss: 0.024130\n",
      "Epoch 1081/3000, Loss: 0.023844\n",
      "Epoch 1091/3000, Loss: 0.023563\n",
      "Epoch 1101/3000, Loss: 0.023287\n",
      "Epoch 1111/3000, Loss: 0.023015\n",
      "Epoch 1121/3000, Loss: 0.022747\n",
      "Epoch 1131/3000, Loss: 0.022484\n",
      "Epoch 1141/3000, Loss: 0.022225\n",
      "Epoch 1151/3000, Loss: 0.021970\n",
      "Epoch 1161/3000, Loss: 0.021719\n",
      "Epoch 1171/3000, Loss: 0.021472\n",
      "Epoch 1181/3000, Loss: 0.021229\n",
      "Epoch 1191/3000, Loss: 0.020989\n",
      "Epoch 1201/3000, Loss: 0.020754\n",
      "Epoch 1211/3000, Loss: 0.020522\n",
      "Epoch 1221/3000, Loss: 0.020293\n",
      "Epoch 1231/3000, Loss: 0.020069\n",
      "Epoch 1241/3000, Loss: 0.019847\n",
      "Epoch 1251/3000, Loss: 0.019629\n",
      "Epoch 1261/3000, Loss: 0.019414\n",
      "Epoch 1271/3000, Loss: 0.019203\n",
      "Epoch 1281/3000, Loss: 0.018995\n",
      "Epoch 1291/3000, Loss: 0.018790\n",
      "Epoch 1301/3000, Loss: 0.018588\n",
      "Epoch 1311/3000, Loss: 0.018389\n",
      "Epoch 1321/3000, Loss: 0.018193\n",
      "Epoch 1331/3000, Loss: 0.018000\n",
      "Epoch 1341/3000, Loss: 0.017810\n",
      "Epoch 1351/3000, Loss: 0.017624\n",
      "Epoch 1361/3000, Loss: 0.017440\n",
      "Epoch 1371/3000, Loss: 0.017258\n",
      "Epoch 1381/3000, Loss: 0.017080\n",
      "Epoch 1391/3000, Loss: 0.016904\n",
      "Epoch 1401/3000, Loss: 0.016732\n",
      "Epoch 1411/3000, Loss: 0.016561\n",
      "Epoch 1421/3000, Loss: 0.016394\n",
      "Epoch 1431/3000, Loss: 0.016229\n",
      "Epoch 1441/3000, Loss: 0.016067\n",
      "Epoch 1451/3000, Loss: 0.015907\n",
      "Epoch 1461/3000, Loss: 0.015750\n",
      "Epoch 1471/3000, Loss: 0.015595\n",
      "Epoch 1481/3000, Loss: 0.015443\n",
      "Epoch 1491/3000, Loss: 0.015293\n",
      "Epoch 1501/3000, Loss: 0.015146\n",
      "Epoch 1511/3000, Loss: 0.015001\n",
      "Epoch 1521/3000, Loss: 0.014858\n",
      "Epoch 1531/3000, Loss: 0.014717\n",
      "Epoch 1541/3000, Loss: 0.014579\n",
      "Epoch 1551/3000, Loss: 0.014443\n",
      "Epoch 1561/3000, Loss: 0.014309\n",
      "Epoch 1571/3000, Loss: 0.014177\n",
      "Epoch 1581/3000, Loss: 0.014047\n",
      "Epoch 1591/3000, Loss: 0.013919\n",
      "Epoch 1601/3000, Loss: 0.013794\n",
      "Epoch 1611/3000, Loss: 0.013670\n",
      "Epoch 1621/3000, Loss: 0.013548\n",
      "Epoch 1631/3000, Loss: 0.013428\n",
      "Epoch 1641/3000, Loss: 0.013309\n",
      "Epoch 1651/3000, Loss: 0.013193\n",
      "Epoch 1661/3000, Loss: 0.013078\n",
      "Epoch 1671/3000, Loss: 0.012965\n",
      "Epoch 1681/3000, Loss: 0.012854\n",
      "Epoch 1691/3000, Loss: 0.012744\n",
      "Epoch 1701/3000, Loss: 0.012636\n",
      "Epoch 1711/3000, Loss: 0.012529\n",
      "Epoch 1721/3000, Loss: 0.012424\n",
      "Epoch 1731/3000, Loss: 0.012320\n",
      "Epoch 1741/3000, Loss: 0.012218\n",
      "Epoch 1751/3000, Loss: 0.012117\n",
      "Epoch 1761/3000, Loss: 0.012017\n",
      "Epoch 1771/3000, Loss: 0.011919\n",
      "Epoch 1781/3000, Loss: 0.011822\n",
      "Epoch 1791/3000, Loss: 0.011727\n",
      "Epoch 1801/3000, Loss: 0.011632\n",
      "Epoch 1811/3000, Loss: 0.011539\n",
      "Epoch 1821/3000, Loss: 0.011447\n",
      "Epoch 1831/3000, Loss: 0.011356\n",
      "Epoch 1841/3000, Loss: 0.011266\n",
      "Epoch 1851/3000, Loss: 0.011177\n",
      "Epoch 1861/3000, Loss: 0.011090\n",
      "Epoch 1871/3000, Loss: 0.011003\n",
      "Epoch 1881/3000, Loss: 0.010917\n",
      "Epoch 1891/3000, Loss: 0.010833\n",
      "Epoch 1901/3000, Loss: 0.010749\n",
      "Epoch 1911/3000, Loss: 0.010666\n",
      "Epoch 1921/3000, Loss: 0.010585\n",
      "Epoch 1931/3000, Loss: 0.010504\n",
      "Epoch 1941/3000, Loss: 0.010424\n",
      "Epoch 1951/3000, Loss: 0.010345\n",
      "Epoch 1961/3000, Loss: 0.010267\n",
      "Epoch 1971/3000, Loss: 0.010189\n",
      "Epoch 1981/3000, Loss: 0.010113\n",
      "Epoch 1991/3000, Loss: 0.010037\n",
      "Epoch 2001/3000, Loss: 0.009962\n",
      "Epoch 2011/3000, Loss: 0.009888\n",
      "Epoch 2021/3000, Loss: 0.009814\n",
      "Epoch 2031/3000, Loss: 0.009742\n",
      "Epoch 2041/3000, Loss: 0.009670\n",
      "Epoch 2051/3000, Loss: 0.009599\n",
      "Epoch 2061/3000, Loss: 0.009528\n",
      "Epoch 2071/3000, Loss: 0.009459\n",
      "Epoch 2081/3000, Loss: 0.009390\n",
      "Epoch 2091/3000, Loss: 0.009321\n",
      "Epoch 2101/3000, Loss: 0.009254\n",
      "Epoch 2111/3000, Loss: 0.009187\n",
      "Epoch 2121/3000, Loss: 0.009121\n",
      "Epoch 2131/3000, Loss: 0.009055\n",
      "Epoch 2141/3000, Loss: 0.008990\n",
      "Epoch 2151/3000, Loss: 0.008926\n",
      "Epoch 2161/3000, Loss: 0.008862\n",
      "Epoch 2171/3000, Loss: 0.008799\n",
      "Epoch 2181/3000, Loss: 0.008736\n",
      "Epoch 2191/3000, Loss: 0.008675\n",
      "Epoch 2201/3000, Loss: 0.008613\n",
      "Epoch 2211/3000, Loss: 0.008553\n",
      "Epoch 2221/3000, Loss: 0.008493\n",
      "Epoch 2231/3000, Loss: 0.008433\n",
      "Epoch 2241/3000, Loss: 0.008374\n",
      "Epoch 2251/3000, Loss: 0.008316\n",
      "Epoch 2261/3000, Loss: 0.008258\n",
      "Epoch 2271/3000, Loss: 0.008201\n",
      "Epoch 2281/3000, Loss: 0.008144\n",
      "Epoch 2291/3000, Loss: 0.008088\n",
      "Epoch 2301/3000, Loss: 0.008032\n",
      "Epoch 2311/3000, Loss: 0.007977\n",
      "Epoch 2321/3000, Loss: 0.007922\n",
      "Epoch 2331/3000, Loss: 0.007868\n",
      "Epoch 2341/3000, Loss: 0.007814\n",
      "Epoch 2351/3000, Loss: 0.007761\n",
      "Epoch 2361/3000, Loss: 0.007708\n",
      "Epoch 2371/3000, Loss: 0.007656\n",
      "Epoch 2381/3000, Loss: 0.007604\n",
      "Epoch 2391/3000, Loss: 0.007553\n",
      "Epoch 2401/3000, Loss: 0.007502\n",
      "Epoch 2411/3000, Loss: 0.007451\n",
      "Epoch 2421/3000, Loss: 0.007401\n",
      "Epoch 2431/3000, Loss: 0.007351\n",
      "Epoch 2441/3000, Loss: 0.007302\n",
      "Epoch 2451/3000, Loss: 0.007253\n",
      "Epoch 2461/3000, Loss: 0.007204\n",
      "Epoch 2471/3000, Loss: 0.007156\n",
      "Epoch 2481/3000, Loss: 0.007108\n",
      "Epoch 2491/3000, Loss: 0.007061\n",
      "Epoch 2501/3000, Loss: 0.007014\n",
      "Epoch 2511/3000, Loss: 0.006967\n",
      "Epoch 2521/3000, Loss: 0.006921\n",
      "Epoch 2531/3000, Loss: 0.006875\n",
      "Epoch 2541/3000, Loss: 0.006829\n",
      "Epoch 2551/3000, Loss: 0.006784\n",
      "Epoch 2561/3000, Loss: 0.006739\n",
      "Epoch 2571/3000, Loss: 0.006694\n",
      "Epoch 2581/3000, Loss: 0.006650\n",
      "Epoch 2591/3000, Loss: 0.006606\n",
      "Epoch 2601/3000, Loss: 0.006562\n",
      "Epoch 2611/3000, Loss: 0.006519\n",
      "Epoch 2621/3000, Loss: 0.006476\n",
      "Epoch 2631/3000, Loss: 0.006433\n",
      "Epoch 2641/3000, Loss: 0.006391\n",
      "Epoch 2651/3000, Loss: 0.006349\n",
      "Epoch 2661/3000, Loss: 0.006307\n",
      "Epoch 2671/3000, Loss: 0.006265\n",
      "Epoch 2681/3000, Loss: 0.006224\n",
      "Epoch 2691/3000, Loss: 0.006183\n",
      "Epoch 2701/3000, Loss: 0.006142\n",
      "Epoch 2711/3000, Loss: 0.006101\n",
      "Epoch 2721/3000, Loss: 0.006061\n",
      "Epoch 2731/3000, Loss: 0.006021\n",
      "Epoch 2741/3000, Loss: 0.005982\n",
      "Epoch 2751/3000, Loss: 0.005942\n",
      "Epoch 2761/3000, Loss: 0.005903\n",
      "Epoch 2771/3000, Loss: 0.005864\n",
      "Epoch 2781/3000, Loss: 0.005826\n",
      "Epoch 2791/3000, Loss: 0.005787\n",
      "Epoch 2801/3000, Loss: 0.005749\n",
      "Epoch 2811/3000, Loss: 0.005711\n",
      "Epoch 2821/3000, Loss: 0.005674\n",
      "Epoch 2831/3000, Loss: 0.005636\n",
      "Epoch 2841/3000, Loss: 0.005599\n",
      "Epoch 2851/3000, Loss: 0.005562\n",
      "Epoch 2861/3000, Loss: 0.005526\n",
      "Epoch 2871/3000, Loss: 0.005489\n",
      "Epoch 2881/3000, Loss: 0.005453\n",
      "Epoch 2891/3000, Loss: 0.005417\n",
      "Epoch 2901/3000, Loss: 0.005382\n",
      "Epoch 2911/3000, Loss: 0.005346\n",
      "Epoch 2921/3000, Loss: 0.005311\n",
      "Epoch 2931/3000, Loss: 0.005276\n",
      "Epoch 2941/3000, Loss: 0.005242\n",
      "Epoch 2951/3000, Loss: 0.005207\n",
      "Epoch 2961/3000, Loss: 0.005173\n",
      "Epoch 2971/3000, Loss: 0.005139\n",
      "Epoch 2981/3000, Loss: 0.005105\n",
      "Epoch 2991/3000, Loss: 0.005072\n",
      "Fine-tuning with Soft-DTW\n",
      "Epoch 1/3000, Loss: 12.545584\n",
      "Converged at epoch 2\n",
      "Euclidean initialization + SoftDTW(gamma=0.1)\n",
      "Pretraining with Euclidean loss\n",
      "Epoch 1/3000, Loss: 0.465915\n",
      "Epoch 11/3000, Loss: 0.243139\n",
      "Epoch 21/3000, Loss: 0.174962\n",
      "Epoch 31/3000, Loss: 0.151466\n",
      "Epoch 41/3000, Loss: 0.139284\n",
      "Epoch 51/3000, Loss: 0.129064\n",
      "Epoch 61/3000, Loss: 0.119909\n",
      "Epoch 71/3000, Loss: 0.111947\n",
      "Epoch 81/3000, Loss: 0.105110\n",
      "Epoch 91/3000, Loss: 0.099276\n",
      "Epoch 101/3000, Loss: 0.094376\n",
      "Epoch 111/3000, Loss: 0.090294\n",
      "Epoch 121/3000, Loss: 0.086903\n",
      "Epoch 131/3000, Loss: 0.084079\n",
      "Epoch 141/3000, Loss: 0.081702\n",
      "Epoch 151/3000, Loss: 0.079668\n",
      "Epoch 161/3000, Loss: 0.077893\n",
      "Epoch 171/3000, Loss: 0.076312\n",
      "Epoch 181/3000, Loss: 0.074877\n",
      "Epoch 191/3000, Loss: 0.073556\n",
      "Epoch 201/3000, Loss: 0.072324\n",
      "Epoch 211/3000, Loss: 0.071167\n",
      "Epoch 221/3000, Loss: 0.070071\n",
      "Epoch 231/3000, Loss: 0.069029\n",
      "Epoch 241/3000, Loss: 0.068032\n",
      "Epoch 251/3000, Loss: 0.067074\n",
      "Epoch 261/3000, Loss: 0.066150\n",
      "Epoch 271/3000, Loss: 0.065254\n",
      "Epoch 281/3000, Loss: 0.064382\n",
      "Epoch 291/3000, Loss: 0.063531\n",
      "Epoch 301/3000, Loss: 0.062698\n",
      "Epoch 311/3000, Loss: 0.061882\n",
      "Epoch 321/3000, Loss: 0.061080\n",
      "Epoch 331/3000, Loss: 0.060291\n",
      "Epoch 341/3000, Loss: 0.059515\n",
      "Epoch 351/3000, Loss: 0.058751\n",
      "Epoch 361/3000, Loss: 0.057997\n",
      "Epoch 371/3000, Loss: 0.057253\n",
      "Epoch 381/3000, Loss: 0.056518\n",
      "Epoch 391/3000, Loss: 0.055793\n",
      "Epoch 401/3000, Loss: 0.055075\n",
      "Epoch 411/3000, Loss: 0.054365\n",
      "Epoch 421/3000, Loss: 0.053662\n",
      "Epoch 431/3000, Loss: 0.052965\n",
      "Epoch 441/3000, Loss: 0.052274\n",
      "Epoch 451/3000, Loss: 0.051589\n",
      "Epoch 461/3000, Loss: 0.050908\n",
      "Epoch 471/3000, Loss: 0.050233\n",
      "Epoch 481/3000, Loss: 0.049562\n",
      "Epoch 491/3000, Loss: 0.048895\n",
      "Epoch 501/3000, Loss: 0.048233\n",
      "Epoch 511/3000, Loss: 0.047576\n",
      "Epoch 521/3000, Loss: 0.046923\n",
      "Epoch 531/3000, Loss: 0.046276\n",
      "Epoch 541/3000, Loss: 0.045634\n",
      "Epoch 551/3000, Loss: 0.044997\n",
      "Epoch 561/3000, Loss: 0.044366\n",
      "Epoch 571/3000, Loss: 0.043742\n",
      "Epoch 581/3000, Loss: 0.043124\n",
      "Epoch 591/3000, Loss: 0.042513\n",
      "Epoch 601/3000, Loss: 0.041909\n",
      "Epoch 611/3000, Loss: 0.041313\n",
      "Epoch 621/3000, Loss: 0.040725\n",
      "Epoch 631/3000, Loss: 0.040144\n",
      "Epoch 641/3000, Loss: 0.039572\n",
      "Epoch 651/3000, Loss: 0.039009\n",
      "Epoch 661/3000, Loss: 0.038453\n",
      "Epoch 671/3000, Loss: 0.037907\n",
      "Epoch 681/3000, Loss: 0.037368\n",
      "Epoch 691/3000, Loss: 0.036838\n",
      "Epoch 701/3000, Loss: 0.036316\n",
      "Epoch 711/3000, Loss: 0.035802\n",
      "Epoch 721/3000, Loss: 0.035296\n",
      "Epoch 731/3000, Loss: 0.034798\n",
      "Epoch 741/3000, Loss: 0.034307\n",
      "Epoch 751/3000, Loss: 0.033823\n",
      "Epoch 761/3000, Loss: 0.033347\n",
      "Epoch 771/3000, Loss: 0.032878\n",
      "Epoch 781/3000, Loss: 0.032415\n",
      "Epoch 791/3000, Loss: 0.031960\n",
      "Epoch 801/3000, Loss: 0.031511\n",
      "Epoch 811/3000, Loss: 0.031068\n",
      "Epoch 821/3000, Loss: 0.030632\n",
      "Epoch 831/3000, Loss: 0.030203\n",
      "Epoch 841/3000, Loss: 0.029779\n",
      "Epoch 851/3000, Loss: 0.029362\n",
      "Epoch 861/3000, Loss: 0.028952\n",
      "Epoch 871/3000, Loss: 0.028547\n",
      "Epoch 881/3000, Loss: 0.028149\n",
      "Epoch 891/3000, Loss: 0.027757\n",
      "Epoch 901/3000, Loss: 0.027372\n",
      "Epoch 911/3000, Loss: 0.026993\n",
      "Epoch 921/3000, Loss: 0.026620\n",
      "Epoch 931/3000, Loss: 0.026254\n",
      "Epoch 941/3000, Loss: 0.025894\n",
      "Epoch 951/3000, Loss: 0.025540\n",
      "Epoch 961/3000, Loss: 0.025193\n",
      "Epoch 971/3000, Loss: 0.024852\n",
      "Epoch 981/3000, Loss: 0.024517\n",
      "Epoch 991/3000, Loss: 0.024189\n",
      "Epoch 1001/3000, Loss: 0.023867\n",
      "Epoch 1011/3000, Loss: 0.023552\n",
      "Epoch 1021/3000, Loss: 0.023242\n",
      "Epoch 1031/3000, Loss: 0.022939\n",
      "Epoch 1041/3000, Loss: 0.022642\n",
      "Epoch 1051/3000, Loss: 0.022351\n",
      "Epoch 1061/3000, Loss: 0.022066\n",
      "Epoch 1071/3000, Loss: 0.021786\n",
      "Epoch 1081/3000, Loss: 0.021513\n",
      "Epoch 1091/3000, Loss: 0.021245\n",
      "Epoch 1101/3000, Loss: 0.020982\n",
      "Epoch 1111/3000, Loss: 0.020725\n",
      "Epoch 1121/3000, Loss: 0.020473\n",
      "Epoch 1131/3000, Loss: 0.020227\n",
      "Epoch 1141/3000, Loss: 0.019985\n",
      "Epoch 1151/3000, Loss: 0.019749\n",
      "Epoch 1161/3000, Loss: 0.019517\n",
      "Epoch 1171/3000, Loss: 0.019290\n",
      "Epoch 1181/3000, Loss: 0.019067\n",
      "Epoch 1191/3000, Loss: 0.018849\n",
      "Epoch 1201/3000, Loss: 0.018635\n",
      "Epoch 1211/3000, Loss: 0.018426\n",
      "Epoch 1221/3000, Loss: 0.018221\n",
      "Epoch 1231/3000, Loss: 0.018019\n",
      "Epoch 1241/3000, Loss: 0.017822\n",
      "Epoch 1251/3000, Loss: 0.017628\n",
      "Epoch 1261/3000, Loss: 0.017438\n",
      "Epoch 1271/3000, Loss: 0.017251\n",
      "Epoch 1281/3000, Loss: 0.017068\n",
      "Epoch 1291/3000, Loss: 0.016888\n",
      "Epoch 1301/3000, Loss: 0.016712\n",
      "Epoch 1311/3000, Loss: 0.016539\n",
      "Epoch 1321/3000, Loss: 0.016369\n",
      "Epoch 1331/3000, Loss: 0.016201\n",
      "Epoch 1341/3000, Loss: 0.016037\n",
      "Epoch 1351/3000, Loss: 0.015876\n",
      "Epoch 1361/3000, Loss: 0.015717\n",
      "Epoch 1371/3000, Loss: 0.015561\n",
      "Epoch 1381/3000, Loss: 0.015408\n",
      "Epoch 1391/3000, Loss: 0.015257\n",
      "Epoch 1401/3000, Loss: 0.015109\n",
      "Epoch 1411/3000, Loss: 0.014963\n",
      "Epoch 1421/3000, Loss: 0.014820\n",
      "Epoch 1431/3000, Loss: 0.014679\n",
      "Epoch 1441/3000, Loss: 0.014540\n",
      "Epoch 1451/3000, Loss: 0.014403\n",
      "Epoch 1461/3000, Loss: 0.014268\n",
      "Epoch 1471/3000, Loss: 0.014136\n",
      "Epoch 1481/3000, Loss: 0.014006\n",
      "Epoch 1491/3000, Loss: 0.013877\n",
      "Epoch 1501/3000, Loss: 0.013751\n",
      "Epoch 1511/3000, Loss: 0.013626\n",
      "Epoch 1521/3000, Loss: 0.013504\n",
      "Epoch 1531/3000, Loss: 0.013383\n",
      "Epoch 1541/3000, Loss: 0.013264\n",
      "Epoch 1551/3000, Loss: 0.013147\n",
      "Epoch 1561/3000, Loss: 0.013031\n",
      "Epoch 1571/3000, Loss: 0.012917\n",
      "Epoch 1581/3000, Loss: 0.012805\n",
      "Epoch 1591/3000, Loss: 0.012695\n",
      "Epoch 1601/3000, Loss: 0.012586\n",
      "Epoch 1611/3000, Loss: 0.012478\n",
      "Epoch 1621/3000, Loss: 0.012373\n",
      "Epoch 1631/3000, Loss: 0.012268\n",
      "Epoch 1641/3000, Loss: 0.012165\n",
      "Epoch 1651/3000, Loss: 0.012064\n",
      "Epoch 1661/3000, Loss: 0.011964\n",
      "Epoch 1671/3000, Loss: 0.011866\n",
      "Epoch 1681/3000, Loss: 0.011768\n",
      "Epoch 1691/3000, Loss: 0.011673\n",
      "Epoch 1701/3000, Loss: 0.011578\n",
      "Epoch 1711/3000, Loss: 0.011485\n",
      "Epoch 1721/3000, Loss: 0.011393\n",
      "Epoch 1731/3000, Loss: 0.011302\n",
      "Epoch 1741/3000, Loss: 0.011213\n",
      "Epoch 1751/3000, Loss: 0.011124\n",
      "Epoch 1761/3000, Loss: 0.011037\n",
      "Epoch 1771/3000, Loss: 0.010951\n",
      "Epoch 1781/3000, Loss: 0.010866\n",
      "Epoch 1791/3000, Loss: 0.010783\n",
      "Epoch 1801/3000, Loss: 0.010700\n",
      "Epoch 1811/3000, Loss: 0.010618\n",
      "Epoch 1821/3000, Loss: 0.010538\n",
      "Epoch 1831/3000, Loss: 0.010458\n",
      "Epoch 1841/3000, Loss: 0.010380\n",
      "Epoch 1851/3000, Loss: 0.010302\n",
      "Epoch 1861/3000, Loss: 0.010225\n",
      "Epoch 1871/3000, Loss: 0.010150\n",
      "Epoch 1881/3000, Loss: 0.010075\n",
      "Epoch 1891/3000, Loss: 0.010001\n",
      "Epoch 1901/3000, Loss: 0.009928\n",
      "Epoch 1911/3000, Loss: 0.009856\n",
      "Epoch 1921/3000, Loss: 0.009784\n",
      "Epoch 1931/3000, Loss: 0.009714\n",
      "Epoch 1941/3000, Loss: 0.009644\n",
      "Epoch 1951/3000, Loss: 0.009575\n",
      "Epoch 1961/3000, Loss: 0.009507\n",
      "Epoch 1971/3000, Loss: 0.009440\n",
      "Epoch 1981/3000, Loss: 0.009373\n",
      "Epoch 1991/3000, Loss: 0.009308\n",
      "Epoch 2001/3000, Loss: 0.009242\n",
      "Epoch 2011/3000, Loss: 0.009178\n",
      "Epoch 2021/3000, Loss: 0.009114\n",
      "Epoch 2031/3000, Loss: 0.009051\n",
      "Epoch 2041/3000, Loss: 0.008989\n",
      "Epoch 2051/3000, Loss: 0.008927\n",
      "Epoch 2061/3000, Loss: 0.008866\n",
      "Epoch 2071/3000, Loss: 0.008806\n",
      "Epoch 2081/3000, Loss: 0.008746\n",
      "Epoch 2091/3000, Loss: 0.008687\n",
      "Epoch 2101/3000, Loss: 0.008629\n",
      "Epoch 2111/3000, Loss: 0.008571\n",
      "Epoch 2121/3000, Loss: 0.008513\n",
      "Epoch 2131/3000, Loss: 0.008457\n",
      "Epoch 2141/3000, Loss: 0.008400\n",
      "Epoch 2151/3000, Loss: 0.008345\n",
      "Epoch 2161/3000, Loss: 0.008289\n",
      "Epoch 2171/3000, Loss: 0.008235\n",
      "Epoch 2181/3000, Loss: 0.008181\n",
      "Epoch 2191/3000, Loss: 0.008127\n",
      "Epoch 2201/3000, Loss: 0.008074\n",
      "Epoch 2211/3000, Loss: 0.008021\n",
      "Epoch 2221/3000, Loss: 0.007969\n",
      "Epoch 2231/3000, Loss: 0.007917\n",
      "Epoch 2241/3000, Loss: 0.007866\n",
      "Epoch 2251/3000, Loss: 0.007815\n",
      "Epoch 2261/3000, Loss: 0.007765\n",
      "Epoch 2271/3000, Loss: 0.007715\n",
      "Epoch 2281/3000, Loss: 0.007665\n",
      "Epoch 2291/3000, Loss: 0.007616\n",
      "Epoch 2301/3000, Loss: 0.007568\n",
      "Epoch 2311/3000, Loss: 0.007519\n",
      "Epoch 2321/3000, Loss: 0.007472\n",
      "Epoch 2331/3000, Loss: 0.007424\n",
      "Epoch 2341/3000, Loss: 0.007377\n",
      "Epoch 2351/3000, Loss: 0.007330\n",
      "Epoch 2361/3000, Loss: 0.007284\n",
      "Epoch 2371/3000, Loss: 0.007238\n",
      "Epoch 2381/3000, Loss: 0.007193\n",
      "Epoch 2391/3000, Loss: 0.007147\n",
      "Epoch 2401/3000, Loss: 0.007103\n",
      "Epoch 2411/3000, Loss: 0.007058\n",
      "Epoch 2421/3000, Loss: 0.007014\n",
      "Epoch 2431/3000, Loss: 0.006970\n",
      "Epoch 2441/3000, Loss: 0.006926\n",
      "Epoch 2451/3000, Loss: 0.006883\n",
      "Epoch 2461/3000, Loss: 0.006840\n",
      "Epoch 2471/3000, Loss: 0.006798\n",
      "Epoch 2481/3000, Loss: 0.006756\n",
      "Epoch 2491/3000, Loss: 0.006714\n",
      "Epoch 2501/3000, Loss: 0.006672\n",
      "Epoch 2511/3000, Loss: 0.006631\n",
      "Epoch 2521/3000, Loss: 0.006590\n",
      "Epoch 2531/3000, Loss: 0.006549\n",
      "Epoch 2541/3000, Loss: 0.006508\n",
      "Epoch 2551/3000, Loss: 0.006468\n",
      "Epoch 2561/3000, Loss: 0.006428\n",
      "Epoch 2571/3000, Loss: 0.006389\n",
      "Epoch 2581/3000, Loss: 0.006349\n",
      "Epoch 2591/3000, Loss: 0.006310\n",
      "Epoch 2601/3000, Loss: 0.006271\n",
      "Epoch 2611/3000, Loss: 0.006233\n",
      "Epoch 2621/3000, Loss: 0.006194\n",
      "Epoch 2631/3000, Loss: 0.006156\n",
      "Epoch 2641/3000, Loss: 0.006118\n",
      "Epoch 2651/3000, Loss: 0.006081\n",
      "Epoch 2661/3000, Loss: 0.006043\n",
      "Epoch 2671/3000, Loss: 0.006006\n",
      "Epoch 2681/3000, Loss: 0.005969\n",
      "Epoch 2691/3000, Loss: 0.005933\n",
      "Epoch 2701/3000, Loss: 0.005896\n",
      "Epoch 2711/3000, Loss: 0.005860\n",
      "Epoch 2721/3000, Loss: 0.005824\n",
      "Epoch 2731/3000, Loss: 0.005788\n",
      "Epoch 2741/3000, Loss: 0.005753\n",
      "Epoch 2751/3000, Loss: 0.005717\n",
      "Epoch 2761/3000, Loss: 0.005682\n",
      "Epoch 2771/3000, Loss: 0.005647\n",
      "Epoch 2781/3000, Loss: 0.005613\n",
      "Epoch 2791/3000, Loss: 0.005578\n",
      "Epoch 2801/3000, Loss: 0.005544\n",
      "Epoch 2811/3000, Loss: 0.005510\n",
      "Epoch 2821/3000, Loss: 0.005476\n",
      "Epoch 2831/3000, Loss: 0.005442\n",
      "Epoch 2841/3000, Loss: 0.005409\n",
      "Epoch 2851/3000, Loss: 0.005375\n",
      "Epoch 2861/3000, Loss: 0.005342\n",
      "Epoch 2871/3000, Loss: 0.005309\n",
      "Epoch 2881/3000, Loss: 0.005276\n",
      "Epoch 2891/3000, Loss: 0.005244\n",
      "Epoch 2901/3000, Loss: 0.005211\n",
      "Epoch 2911/3000, Loss: 0.005179\n",
      "Epoch 2921/3000, Loss: 0.005147\n",
      "Epoch 2931/3000, Loss: 0.005115\n",
      "Epoch 2941/3000, Loss: 0.005083\n",
      "Epoch 2951/3000, Loss: 0.005052\n",
      "Epoch 2961/3000, Loss: 0.005020\n",
      "Epoch 2971/3000, Loss: 0.004989\n",
      "Epoch 2981/3000, Loss: 0.004958\n",
      "Epoch 2991/3000, Loss: 0.004927\n",
      "Fine-tuning with Soft-DTW\n",
      "Epoch 1/3000, Loss: 19.096182\n",
      "Converged at epoch 3\n",
      "Euclidean initialization + SoftDTW(gamma=0.01)\n",
      "Pretraining with Euclidean loss\n",
      "Epoch 1/3000, Loss: 0.455780\n",
      "Epoch 11/3000, Loss: 0.227514\n",
      "Epoch 21/3000, Loss: 0.160256\n",
      "Epoch 31/3000, Loss: 0.144571\n",
      "Epoch 41/3000, Loss: 0.130863\n",
      "Epoch 51/3000, Loss: 0.120250\n",
      "Epoch 61/3000, Loss: 0.111601\n",
      "Epoch 71/3000, Loss: 0.104441\n",
      "Epoch 81/3000, Loss: 0.098624\n",
      "Epoch 91/3000, Loss: 0.093913\n",
      "Epoch 101/3000, Loss: 0.090033\n",
      "Epoch 111/3000, Loss: 0.086806\n",
      "Epoch 121/3000, Loss: 0.084087\n",
      "Epoch 131/3000, Loss: 0.081769\n",
      "Epoch 141/3000, Loss: 0.079764\n",
      "Epoch 151/3000, Loss: 0.078006\n",
      "Epoch 161/3000, Loss: 0.076440\n",
      "Epoch 171/3000, Loss: 0.075024\n",
      "Epoch 181/3000, Loss: 0.073723\n",
      "Epoch 191/3000, Loss: 0.072509\n",
      "Epoch 201/3000, Loss: 0.071363\n",
      "Epoch 211/3000, Loss: 0.070271\n",
      "Epoch 221/3000, Loss: 0.069223\n",
      "Epoch 231/3000, Loss: 0.068213\n",
      "Epoch 241/3000, Loss: 0.067236\n",
      "Epoch 251/3000, Loss: 0.066288\n",
      "Epoch 261/3000, Loss: 0.065368\n",
      "Epoch 271/3000, Loss: 0.064472\n",
      "Epoch 281/3000, Loss: 0.063598\n",
      "Epoch 291/3000, Loss: 0.062744\n",
      "Epoch 301/3000, Loss: 0.061908\n",
      "Epoch 311/3000, Loss: 0.061088\n",
      "Epoch 321/3000, Loss: 0.060283\n",
      "Epoch 331/3000, Loss: 0.059490\n",
      "Epoch 341/3000, Loss: 0.058708\n",
      "Epoch 351/3000, Loss: 0.057937\n",
      "Epoch 361/3000, Loss: 0.057174\n",
      "Epoch 371/3000, Loss: 0.056420\n",
      "Epoch 381/3000, Loss: 0.055674\n",
      "Epoch 391/3000, Loss: 0.054934\n",
      "Epoch 401/3000, Loss: 0.054201\n",
      "Epoch 411/3000, Loss: 0.053474\n",
      "Epoch 421/3000, Loss: 0.052753\n",
      "Epoch 431/3000, Loss: 0.052037\n",
      "Epoch 441/3000, Loss: 0.051327\n",
      "Epoch 451/3000, Loss: 0.050622\n",
      "Epoch 461/3000, Loss: 0.049923\n",
      "Epoch 471/3000, Loss: 0.049230\n",
      "Epoch 481/3000, Loss: 0.048542\n",
      "Epoch 491/3000, Loss: 0.047861\n",
      "Epoch 501/3000, Loss: 0.047186\n",
      "Epoch 511/3000, Loss: 0.046517\n",
      "Epoch 521/3000, Loss: 0.045856\n",
      "Epoch 531/3000, Loss: 0.045203\n",
      "Epoch 541/3000, Loss: 0.044557\n",
      "Epoch 551/3000, Loss: 0.043920\n",
      "Epoch 561/3000, Loss: 0.043292\n",
      "Epoch 571/3000, Loss: 0.042672\n",
      "Epoch 581/3000, Loss: 0.042062\n",
      "Epoch 591/3000, Loss: 0.041460\n",
      "Epoch 601/3000, Loss: 0.040866\n",
      "Epoch 611/3000, Loss: 0.040281\n",
      "Epoch 621/3000, Loss: 0.039704\n",
      "Epoch 631/3000, Loss: 0.039136\n",
      "Epoch 641/3000, Loss: 0.038575\n",
      "Epoch 651/3000, Loss: 0.038024\n",
      "Epoch 661/3000, Loss: 0.037480\n",
      "Epoch 671/3000, Loss: 0.036945\n",
      "Epoch 681/3000, Loss: 0.036418\n",
      "Epoch 691/3000, Loss: 0.035900\n",
      "Epoch 701/3000, Loss: 0.035391\n",
      "Epoch 711/3000, Loss: 0.034890\n",
      "Epoch 721/3000, Loss: 0.034397\n",
      "Epoch 731/3000, Loss: 0.033912\n",
      "Epoch 741/3000, Loss: 0.033436\n",
      "Epoch 751/3000, Loss: 0.032967\n",
      "Epoch 761/3000, Loss: 0.032507\n",
      "Epoch 771/3000, Loss: 0.032054\n",
      "Epoch 781/3000, Loss: 0.031609\n",
      "Epoch 791/3000, Loss: 0.031171\n",
      "Epoch 801/3000, Loss: 0.030742\n",
      "Epoch 811/3000, Loss: 0.030319\n",
      "Epoch 821/3000, Loss: 0.029905\n",
      "Epoch 831/3000, Loss: 0.029497\n",
      "Epoch 841/3000, Loss: 0.029098\n",
      "Epoch 851/3000, Loss: 0.028705\n",
      "Epoch 861/3000, Loss: 0.028320\n",
      "Epoch 871/3000, Loss: 0.027942\n",
      "Epoch 881/3000, Loss: 0.027571\n",
      "Epoch 891/3000, Loss: 0.027207\n",
      "Epoch 901/3000, Loss: 0.026849\n",
      "Epoch 911/3000, Loss: 0.026498\n",
      "Epoch 921/3000, Loss: 0.026154\n",
      "Epoch 931/3000, Loss: 0.025816\n",
      "Epoch 941/3000, Loss: 0.025483\n",
      "Epoch 951/3000, Loss: 0.025157\n",
      "Epoch 961/3000, Loss: 0.024836\n",
      "Epoch 971/3000, Loss: 0.024521\n",
      "Epoch 981/3000, Loss: 0.024212\n",
      "Epoch 991/3000, Loss: 0.023907\n",
      "Epoch 1001/3000, Loss: 0.023608\n",
      "Epoch 1011/3000, Loss: 0.023314\n",
      "Epoch 1021/3000, Loss: 0.023025\n",
      "Epoch 1031/3000, Loss: 0.022741\n",
      "Epoch 1041/3000, Loss: 0.022461\n",
      "Epoch 1051/3000, Loss: 0.022186\n",
      "Epoch 1061/3000, Loss: 0.021915\n",
      "Epoch 1071/3000, Loss: 0.021649\n",
      "Epoch 1081/3000, Loss: 0.021387\n",
      "Epoch 1091/3000, Loss: 0.021129\n",
      "Epoch 1101/3000, Loss: 0.020875\n",
      "Epoch 1111/3000, Loss: 0.020625\n",
      "Epoch 1121/3000, Loss: 0.020379\n",
      "Epoch 1131/3000, Loss: 0.020137\n",
      "Epoch 1141/3000, Loss: 0.019899\n",
      "Epoch 1151/3000, Loss: 0.019664\n",
      "Epoch 1161/3000, Loss: 0.019434\n",
      "Epoch 1171/3000, Loss: 0.019207\n",
      "Epoch 1181/3000, Loss: 0.018983\n",
      "Epoch 1191/3000, Loss: 0.018763\n",
      "Epoch 1201/3000, Loss: 0.018547\n",
      "Epoch 1211/3000, Loss: 0.018334\n",
      "Epoch 1221/3000, Loss: 0.018125\n",
      "Epoch 1231/3000, Loss: 0.017919\n",
      "Epoch 1241/3000, Loss: 0.017716\n",
      "Epoch 1251/3000, Loss: 0.017517\n",
      "Epoch 1261/3000, Loss: 0.017321\n",
      "Epoch 1271/3000, Loss: 0.017129\n",
      "Epoch 1281/3000, Loss: 0.016940\n",
      "Epoch 1291/3000, Loss: 0.016754\n",
      "Epoch 1301/3000, Loss: 0.016571\n",
      "Epoch 1311/3000, Loss: 0.016391\n",
      "Epoch 1321/3000, Loss: 0.016214\n",
      "Epoch 1331/3000, Loss: 0.016040\n",
      "Epoch 1341/3000, Loss: 0.015870\n",
      "Epoch 1351/3000, Loss: 0.015702\n",
      "Epoch 1361/3000, Loss: 0.015537\n",
      "Epoch 1371/3000, Loss: 0.015374\n",
      "Epoch 1381/3000, Loss: 0.015215\n",
      "Epoch 1391/3000, Loss: 0.015058\n",
      "Epoch 1401/3000, Loss: 0.014904\n",
      "Epoch 1411/3000, Loss: 0.014752\n",
      "Epoch 1421/3000, Loss: 0.014603\n",
      "Epoch 1431/3000, Loss: 0.014456\n",
      "Epoch 1441/3000, Loss: 0.014312\n",
      "Epoch 1451/3000, Loss: 0.014170\n",
      "Epoch 1461/3000, Loss: 0.014030\n",
      "Epoch 1471/3000, Loss: 0.013893\n",
      "Epoch 1481/3000, Loss: 0.013758\n",
      "Epoch 1491/3000, Loss: 0.013624\n",
      "Epoch 1501/3000, Loss: 0.013493\n",
      "Epoch 1511/3000, Loss: 0.013364\n",
      "Epoch 1521/3000, Loss: 0.013237\n",
      "Epoch 1531/3000, Loss: 0.013112\n",
      "Epoch 1541/3000, Loss: 0.012989\n",
      "Epoch 1551/3000, Loss: 0.012868\n",
      "Epoch 1561/3000, Loss: 0.012748\n",
      "Epoch 1571/3000, Loss: 0.012631\n",
      "Epoch 1581/3000, Loss: 0.012515\n",
      "Epoch 1591/3000, Loss: 0.012401\n",
      "Epoch 1601/3000, Loss: 0.012288\n",
      "Epoch 1611/3000, Loss: 0.012177\n",
      "Epoch 1621/3000, Loss: 0.012068\n",
      "Epoch 1631/3000, Loss: 0.011960\n",
      "Epoch 1641/3000, Loss: 0.011854\n",
      "Epoch 1651/3000, Loss: 0.011749\n",
      "Epoch 1661/3000, Loss: 0.011646\n",
      "Epoch 1671/3000, Loss: 0.011544\n",
      "Epoch 1681/3000, Loss: 0.011444\n",
      "Epoch 1691/3000, Loss: 0.011345\n",
      "Epoch 1701/3000, Loss: 0.011248\n",
      "Epoch 1711/3000, Loss: 0.011151\n",
      "Epoch 1721/3000, Loss: 0.011056\n",
      "Epoch 1731/3000, Loss: 0.010963\n",
      "Epoch 1741/3000, Loss: 0.010870\n",
      "Epoch 1751/3000, Loss: 0.010779\n",
      "Epoch 1761/3000, Loss: 0.010689\n",
      "Epoch 1771/3000, Loss: 0.010600\n",
      "Epoch 1781/3000, Loss: 0.010513\n",
      "Epoch 1791/3000, Loss: 0.010426\n",
      "Epoch 1801/3000, Loss: 0.010341\n",
      "Epoch 1811/3000, Loss: 0.010257\n",
      "Epoch 1821/3000, Loss: 0.010174\n",
      "Epoch 1831/3000, Loss: 0.010092\n",
      "Epoch 1841/3000, Loss: 0.010011\n",
      "Epoch 1851/3000, Loss: 0.009931\n",
      "Epoch 1861/3000, Loss: 0.009852\n",
      "Epoch 1871/3000, Loss: 0.009774\n",
      "Epoch 1881/3000, Loss: 0.009696\n",
      "Epoch 1891/3000, Loss: 0.009620\n",
      "Epoch 1901/3000, Loss: 0.009545\n",
      "Epoch 1911/3000, Loss: 0.009471\n",
      "Epoch 1921/3000, Loss: 0.009398\n",
      "Epoch 1931/3000, Loss: 0.009325\n",
      "Epoch 1941/3000, Loss: 0.009254\n",
      "Epoch 1951/3000, Loss: 0.009183\n",
      "Epoch 1961/3000, Loss: 0.009113\n",
      "Epoch 1971/3000, Loss: 0.009044\n",
      "Epoch 1981/3000, Loss: 0.008976\n",
      "Epoch 1991/3000, Loss: 0.008908\n",
      "Epoch 2001/3000, Loss: 0.008842\n",
      "Epoch 2011/3000, Loss: 0.008776\n",
      "Epoch 2021/3000, Loss: 0.008711\n",
      "Epoch 2031/3000, Loss: 0.008646\n",
      "Epoch 2041/3000, Loss: 0.008583\n",
      "Epoch 2051/3000, Loss: 0.008520\n",
      "Epoch 2061/3000, Loss: 0.008458\n",
      "Epoch 2071/3000, Loss: 0.008396\n",
      "Epoch 2081/3000, Loss: 0.008335\n",
      "Epoch 2091/3000, Loss: 0.008275\n",
      "Epoch 2101/3000, Loss: 0.008216\n",
      "Epoch 2111/3000, Loss: 0.008157\n",
      "Epoch 2121/3000, Loss: 0.008099\n",
      "Epoch 2131/3000, Loss: 0.008041\n",
      "Epoch 2141/3000, Loss: 0.007984\n",
      "Epoch 2151/3000, Loss: 0.007928\n",
      "Epoch 2161/3000, Loss: 0.007872\n",
      "Epoch 2171/3000, Loss: 0.007817\n",
      "Epoch 2181/3000, Loss: 0.007763\n",
      "Epoch 2191/3000, Loss: 0.007709\n",
      "Epoch 2201/3000, Loss: 0.007655\n",
      "Epoch 2211/3000, Loss: 0.007602\n",
      "Epoch 2221/3000, Loss: 0.007550\n",
      "Epoch 2231/3000, Loss: 0.007498\n",
      "Epoch 2241/3000, Loss: 0.007447\n",
      "Epoch 2251/3000, Loss: 0.007396\n",
      "Epoch 2261/3000, Loss: 0.007346\n",
      "Epoch 2271/3000, Loss: 0.007296\n",
      "Epoch 2281/3000, Loss: 0.007247\n",
      "Epoch 2291/3000, Loss: 0.007198\n",
      "Epoch 2301/3000, Loss: 0.007149\n",
      "Epoch 2311/3000, Loss: 0.007101\n",
      "Epoch 2321/3000, Loss: 0.007054\n",
      "Epoch 2331/3000, Loss: 0.007007\n",
      "Epoch 2341/3000, Loss: 0.006960\n",
      "Epoch 2351/3000, Loss: 0.006914\n",
      "Epoch 2361/3000, Loss: 0.006868\n",
      "Epoch 2371/3000, Loss: 0.006823\n",
      "Epoch 2381/3000, Loss: 0.006778\n",
      "Epoch 2391/3000, Loss: 0.006733\n",
      "Epoch 2401/3000, Loss: 0.006689\n",
      "Epoch 2411/3000, Loss: 0.006645\n",
      "Epoch 2421/3000, Loss: 0.006602\n",
      "Epoch 2431/3000, Loss: 0.006559\n",
      "Epoch 2441/3000, Loss: 0.006516\n",
      "Epoch 2451/3000, Loss: 0.006474\n",
      "Epoch 2461/3000, Loss: 0.006432\n",
      "Epoch 2471/3000, Loss: 0.006390\n",
      "Epoch 2481/3000, Loss: 0.006348\n",
      "Epoch 2491/3000, Loss: 0.006307\n",
      "Epoch 2501/3000, Loss: 0.006267\n",
      "Epoch 2511/3000, Loss: 0.006226\n",
      "Epoch 2521/3000, Loss: 0.006186\n",
      "Epoch 2531/3000, Loss: 0.006146\n",
      "Epoch 2541/3000, Loss: 0.006107\n",
      "Epoch 2551/3000, Loss: 0.006068\n",
      "Epoch 2561/3000, Loss: 0.006029\n",
      "Epoch 2571/3000, Loss: 0.005990\n",
      "Epoch 2581/3000, Loss: 0.005952\n",
      "Epoch 2591/3000, Loss: 0.005914\n",
      "Epoch 2601/3000, Loss: 0.005876\n",
      "Epoch 2611/3000, Loss: 0.005838\n",
      "Epoch 2621/3000, Loss: 0.005801\n",
      "Epoch 2631/3000, Loss: 0.005764\n",
      "Epoch 2641/3000, Loss: 0.005727\n",
      "Epoch 2651/3000, Loss: 0.005691\n",
      "Epoch 2661/3000, Loss: 0.005654\n",
      "Epoch 2671/3000, Loss: 0.005618\n",
      "Epoch 2681/3000, Loss: 0.005583\n",
      "Epoch 2691/3000, Loss: 0.005547\n",
      "Epoch 2701/3000, Loss: 0.005512\n",
      "Epoch 2711/3000, Loss: 0.005477\n",
      "Epoch 2721/3000, Loss: 0.005442\n",
      "Epoch 2731/3000, Loss: 0.005407\n",
      "Epoch 2741/3000, Loss: 0.005373\n",
      "Epoch 2751/3000, Loss: 0.005338\n",
      "Epoch 2761/3000, Loss: 0.005304\n",
      "Epoch 2771/3000, Loss: 0.005271\n",
      "Epoch 2781/3000, Loss: 0.005237\n",
      "Epoch 2791/3000, Loss: 0.005204\n",
      "Epoch 2801/3000, Loss: 0.005171\n",
      "Epoch 2811/3000, Loss: 0.005138\n",
      "Epoch 2821/3000, Loss: 0.005105\n",
      "Epoch 2831/3000, Loss: 0.005072\n",
      "Epoch 2841/3000, Loss: 0.005040\n",
      "Epoch 2851/3000, Loss: 0.005008\n",
      "Epoch 2861/3000, Loss: 0.004976\n",
      "Epoch 2871/3000, Loss: 0.004944\n",
      "Epoch 2881/3000, Loss: 0.004912\n",
      "Epoch 2891/3000, Loss: 0.004881\n",
      "Epoch 2901/3000, Loss: 0.004849\n",
      "Epoch 2911/3000, Loss: 0.004818\n",
      "Epoch 2921/3000, Loss: 0.004787\n",
      "Epoch 2931/3000, Loss: 0.004757\n",
      "Epoch 2941/3000, Loss: 0.004726\n",
      "Epoch 2951/3000, Loss: 0.004696\n",
      "Epoch 2961/3000, Loss: 0.004666\n",
      "Epoch 2971/3000, Loss: 0.004636\n",
      "Epoch 2981/3000, Loss: 0.004606\n",
      "Epoch 2991/3000, Loss: 0.004576\n",
      "Fine-tuning with Soft-DTW\n",
      "Epoch 1/3000, Loss: 17.731199\n",
      "Converged at epoch 3\n",
      "Euclidean initialization + SoftDTW(gamma=0.001)\n",
      "Pretraining with Euclidean loss\n",
      "Epoch 1/3000, Loss: 0.426614\n",
      "Epoch 11/3000, Loss: 0.220988\n",
      "Epoch 21/3000, Loss: 0.161612\n",
      "Epoch 31/3000, Loss: 0.145491\n",
      "Epoch 41/3000, Loss: 0.133501\n",
      "Epoch 51/3000, Loss: 0.122526\n",
      "Epoch 61/3000, Loss: 0.113346\n",
      "Epoch 71/3000, Loss: 0.105521\n",
      "Epoch 81/3000, Loss: 0.098974\n",
      "Epoch 91/3000, Loss: 0.093675\n",
      "Epoch 101/3000, Loss: 0.089412\n",
      "Epoch 111/3000, Loss: 0.085950\n",
      "Epoch 121/3000, Loss: 0.083084\n",
      "Epoch 131/3000, Loss: 0.080659\n",
      "Epoch 141/3000, Loss: 0.078569\n",
      "Epoch 151/3000, Loss: 0.076738\n",
      "Epoch 161/3000, Loss: 0.075109\n",
      "Epoch 171/3000, Loss: 0.073639\n",
      "Epoch 181/3000, Loss: 0.072297\n",
      "Epoch 191/3000, Loss: 0.071057\n",
      "Epoch 201/3000, Loss: 0.069902\n",
      "Epoch 211/3000, Loss: 0.068815\n",
      "Epoch 221/3000, Loss: 0.067784\n",
      "Epoch 231/3000, Loss: 0.066801\n",
      "Epoch 241/3000, Loss: 0.065858\n",
      "Epoch 251/3000, Loss: 0.064947\n",
      "Epoch 261/3000, Loss: 0.064065\n",
      "Epoch 271/3000, Loss: 0.063206\n",
      "Epoch 281/3000, Loss: 0.062367\n",
      "Epoch 291/3000, Loss: 0.061546\n",
      "Epoch 301/3000, Loss: 0.060739\n",
      "Epoch 311/3000, Loss: 0.059945\n",
      "Epoch 321/3000, Loss: 0.059162\n",
      "Epoch 331/3000, Loss: 0.058389\n",
      "Epoch 341/3000, Loss: 0.057625\n",
      "Epoch 351/3000, Loss: 0.056868\n",
      "Epoch 361/3000, Loss: 0.056119\n",
      "Epoch 371/3000, Loss: 0.055375\n",
      "Epoch 381/3000, Loss: 0.054636\n",
      "Epoch 391/3000, Loss: 0.053901\n",
      "Epoch 401/3000, Loss: 0.053170\n",
      "Epoch 411/3000, Loss: 0.052441\n",
      "Epoch 421/3000, Loss: 0.051714\n",
      "Epoch 431/3000, Loss: 0.050989\n",
      "Epoch 441/3000, Loss: 0.050266\n",
      "Epoch 451/3000, Loss: 0.049544\n",
      "Epoch 461/3000, Loss: 0.048824\n",
      "Epoch 471/3000, Loss: 0.048105\n",
      "Epoch 481/3000, Loss: 0.047388\n",
      "Epoch 491/3000, Loss: 0.046673\n",
      "Epoch 501/3000, Loss: 0.045962\n",
      "Epoch 511/3000, Loss: 0.045253\n",
      "Epoch 521/3000, Loss: 0.044548\n",
      "Epoch 531/3000, Loss: 0.043847\n",
      "Epoch 541/3000, Loss: 0.043152\n",
      "Epoch 551/3000, Loss: 0.042463\n",
      "Epoch 561/3000, Loss: 0.041780\n",
      "Epoch 571/3000, Loss: 0.041106\n",
      "Epoch 581/3000, Loss: 0.040439\n",
      "Epoch 591/3000, Loss: 0.039782\n",
      "Epoch 601/3000, Loss: 0.039135\n",
      "Epoch 611/3000, Loss: 0.038498\n",
      "Epoch 621/3000, Loss: 0.037873\n",
      "Epoch 631/3000, Loss: 0.037258\n",
      "Epoch 641/3000, Loss: 0.036656\n",
      "Epoch 651/3000, Loss: 0.036065\n",
      "Epoch 661/3000, Loss: 0.035485\n",
      "Epoch 671/3000, Loss: 0.034918\n",
      "Epoch 681/3000, Loss: 0.034362\n",
      "Epoch 691/3000, Loss: 0.033818\n",
      "Epoch 701/3000, Loss: 0.033285\n",
      "Epoch 711/3000, Loss: 0.032763\n",
      "Epoch 721/3000, Loss: 0.032252\n",
      "Epoch 731/3000, Loss: 0.031751\n",
      "Epoch 741/3000, Loss: 0.031261\n",
      "Epoch 751/3000, Loss: 0.030781\n",
      "Epoch 761/3000, Loss: 0.030311\n",
      "Epoch 771/3000, Loss: 0.029850\n",
      "Epoch 781/3000, Loss: 0.029399\n",
      "Epoch 791/3000, Loss: 0.028957\n",
      "Epoch 801/3000, Loss: 0.028525\n",
      "Epoch 811/3000, Loss: 0.028101\n",
      "Epoch 821/3000, Loss: 0.027686\n",
      "Epoch 831/3000, Loss: 0.027279\n",
      "Epoch 841/3000, Loss: 0.026880\n",
      "Epoch 851/3000, Loss: 0.026490\n",
      "Epoch 861/3000, Loss: 0.026107\n",
      "Epoch 871/3000, Loss: 0.025732\n",
      "Epoch 881/3000, Loss: 0.025365\n",
      "Epoch 891/3000, Loss: 0.025005\n",
      "Epoch 901/3000, Loss: 0.024653\n",
      "Epoch 911/3000, Loss: 0.024308\n",
      "Epoch 921/3000, Loss: 0.023970\n",
      "Epoch 931/3000, Loss: 0.023638\n",
      "Epoch 941/3000, Loss: 0.023314\n",
      "Epoch 951/3000, Loss: 0.022996\n",
      "Epoch 961/3000, Loss: 0.022685\n",
      "Epoch 971/3000, Loss: 0.022380\n",
      "Epoch 981/3000, Loss: 0.022082\n",
      "Epoch 991/3000, Loss: 0.021790\n",
      "Epoch 1001/3000, Loss: 0.021504\n",
      "Epoch 1011/3000, Loss: 0.021224\n",
      "Epoch 1021/3000, Loss: 0.020950\n",
      "Epoch 1031/3000, Loss: 0.020682\n",
      "Epoch 1041/3000, Loss: 0.020419\n",
      "Epoch 1051/3000, Loss: 0.020162\n",
      "Epoch 1061/3000, Loss: 0.019910\n",
      "Epoch 1071/3000, Loss: 0.019663\n",
      "Epoch 1081/3000, Loss: 0.019421\n",
      "Epoch 1091/3000, Loss: 0.019185\n",
      "Epoch 1101/3000, Loss: 0.018953\n",
      "Epoch 1111/3000, Loss: 0.018726\n",
      "Epoch 1121/3000, Loss: 0.018504\n",
      "Epoch 1131/3000, Loss: 0.018286\n",
      "Epoch 1141/3000, Loss: 0.018072\n",
      "Epoch 1151/3000, Loss: 0.017863\n",
      "Epoch 1161/3000, Loss: 0.017659\n",
      "Epoch 1171/3000, Loss: 0.017458\n",
      "Epoch 1181/3000, Loss: 0.017261\n",
      "Epoch 1191/3000, Loss: 0.017068\n",
      "Epoch 1201/3000, Loss: 0.016879\n",
      "Epoch 1211/3000, Loss: 0.016694\n",
      "Epoch 1221/3000, Loss: 0.016512\n",
      "Epoch 1231/3000, Loss: 0.016334\n",
      "Epoch 1241/3000, Loss: 0.016159\n",
      "Epoch 1251/3000, Loss: 0.015987\n",
      "Epoch 1261/3000, Loss: 0.015818\n",
      "Epoch 1271/3000, Loss: 0.015653\n",
      "Epoch 1281/3000, Loss: 0.015490\n",
      "Epoch 1291/3000, Loss: 0.015331\n",
      "Epoch 1301/3000, Loss: 0.015174\n",
      "Epoch 1311/3000, Loss: 0.015020\n",
      "Epoch 1321/3000, Loss: 0.014869\n",
      "Epoch 1331/3000, Loss: 0.014721\n",
      "Epoch 1341/3000, Loss: 0.014575\n",
      "Epoch 1351/3000, Loss: 0.014431\n",
      "Epoch 1361/3000, Loss: 0.014290\n",
      "Epoch 1371/3000, Loss: 0.014151\n",
      "Epoch 1381/3000, Loss: 0.014015\n",
      "Epoch 1391/3000, Loss: 0.013881\n",
      "Epoch 1401/3000, Loss: 0.013749\n",
      "Epoch 1411/3000, Loss: 0.013619\n",
      "Epoch 1421/3000, Loss: 0.013491\n",
      "Epoch 1431/3000, Loss: 0.013365\n",
      "Epoch 1441/3000, Loss: 0.013242\n",
      "Epoch 1451/3000, Loss: 0.013120\n",
      "Epoch 1461/3000, Loss: 0.013000\n",
      "Epoch 1471/3000, Loss: 0.012881\n",
      "Epoch 1481/3000, Loss: 0.012765\n",
      "Epoch 1491/3000, Loss: 0.012650\n",
      "Epoch 1501/3000, Loss: 0.012537\n",
      "Epoch 1511/3000, Loss: 0.012425\n",
      "Epoch 1521/3000, Loss: 0.012316\n",
      "Epoch 1531/3000, Loss: 0.012207\n",
      "Epoch 1541/3000, Loss: 0.012100\n",
      "Epoch 1551/3000, Loss: 0.011995\n",
      "Epoch 1561/3000, Loss: 0.011891\n",
      "Epoch 1571/3000, Loss: 0.011788\n",
      "Epoch 1581/3000, Loss: 0.011687\n",
      "Epoch 1591/3000, Loss: 0.011587\n",
      "Epoch 1601/3000, Loss: 0.011488\n",
      "Epoch 1611/3000, Loss: 0.011390\n",
      "Epoch 1621/3000, Loss: 0.011294\n",
      "Epoch 1631/3000, Loss: 0.011199\n",
      "Epoch 1641/3000, Loss: 0.011105\n",
      "Epoch 1651/3000, Loss: 0.011012\n",
      "Epoch 1661/3000, Loss: 0.010921\n",
      "Epoch 1671/3000, Loss: 0.010830\n",
      "Epoch 1681/3000, Loss: 0.010740\n",
      "Epoch 1691/3000, Loss: 0.010652\n",
      "Epoch 1701/3000, Loss: 0.010564\n",
      "Epoch 1711/3000, Loss: 0.010478\n",
      "Epoch 1721/3000, Loss: 0.010392\n",
      "Epoch 1731/3000, Loss: 0.010308\n",
      "Epoch 1741/3000, Loss: 0.010224\n",
      "Epoch 1751/3000, Loss: 0.010141\n",
      "Epoch 1761/3000, Loss: 0.010060\n",
      "Epoch 1771/3000, Loss: 0.009979\n",
      "Epoch 1781/3000, Loss: 0.009899\n",
      "Epoch 1791/3000, Loss: 0.009820\n",
      "Epoch 1801/3000, Loss: 0.009741\n",
      "Epoch 1811/3000, Loss: 0.009664\n",
      "Epoch 1821/3000, Loss: 0.009587\n",
      "Epoch 1831/3000, Loss: 0.009511\n",
      "Epoch 1841/3000, Loss: 0.009436\n",
      "Epoch 1851/3000, Loss: 0.009362\n",
      "Epoch 1861/3000, Loss: 0.009288\n",
      "Epoch 1871/3000, Loss: 0.009215\n",
      "Epoch 1881/3000, Loss: 0.009143\n",
      "Epoch 1891/3000, Loss: 0.009072\n",
      "Epoch 1901/3000, Loss: 0.009002\n",
      "Epoch 1911/3000, Loss: 0.008932\n",
      "Epoch 1921/3000, Loss: 0.008863\n",
      "Epoch 1931/3000, Loss: 0.008794\n",
      "Epoch 1941/3000, Loss: 0.008726\n",
      "Epoch 1951/3000, Loss: 0.008659\n",
      "Epoch 1961/3000, Loss: 0.008593\n",
      "Epoch 1971/3000, Loss: 0.008527\n",
      "Epoch 1981/3000, Loss: 0.008462\n",
      "Epoch 1991/3000, Loss: 0.008398\n",
      "Epoch 2001/3000, Loss: 0.008334\n",
      "Epoch 2011/3000, Loss: 0.008271\n",
      "Epoch 2021/3000, Loss: 0.008208\n",
      "Epoch 2031/3000, Loss: 0.008146\n",
      "Epoch 2041/3000, Loss: 0.008085\n",
      "Epoch 2051/3000, Loss: 0.008025\n",
      "Epoch 2061/3000, Loss: 0.007964\n",
      "Epoch 2071/3000, Loss: 0.007905\n",
      "Epoch 2081/3000, Loss: 0.007846\n",
      "Epoch 2091/3000, Loss: 0.007788\n",
      "Epoch 2101/3000, Loss: 0.007730\n",
      "Epoch 2111/3000, Loss: 0.007673\n",
      "Epoch 2121/3000, Loss: 0.007616\n",
      "Epoch 2131/3000, Loss: 0.007560\n",
      "Epoch 2141/3000, Loss: 0.007505\n",
      "Epoch 2151/3000, Loss: 0.007450\n",
      "Epoch 2161/3000, Loss: 0.007396\n",
      "Epoch 2171/3000, Loss: 0.007342\n",
      "Epoch 2181/3000, Loss: 0.007288\n",
      "Epoch 2191/3000, Loss: 0.007235\n",
      "Epoch 2201/3000, Loss: 0.007183\n",
      "Epoch 2211/3000, Loss: 0.007131\n",
      "Epoch 2221/3000, Loss: 0.007080\n",
      "Epoch 2231/3000, Loss: 0.007029\n",
      "Epoch 2241/3000, Loss: 0.006979\n",
      "Epoch 2251/3000, Loss: 0.006929\n",
      "Epoch 2261/3000, Loss: 0.006879\n",
      "Epoch 2271/3000, Loss: 0.006830\n",
      "Epoch 2281/3000, Loss: 0.006782\n",
      "Epoch 2291/3000, Loss: 0.006734\n",
      "Epoch 2301/3000, Loss: 0.006686\n",
      "Epoch 2311/3000, Loss: 0.006639\n",
      "Epoch 2321/3000, Loss: 0.006592\n",
      "Epoch 2331/3000, Loss: 0.006546\n",
      "Epoch 2341/3000, Loss: 0.006500\n",
      "Epoch 2351/3000, Loss: 0.006454\n",
      "Epoch 2361/3000, Loss: 0.006409\n",
      "Epoch 2371/3000, Loss: 0.006364\n",
      "Epoch 2381/3000, Loss: 0.006320\n",
      "Epoch 2391/3000, Loss: 0.006276\n",
      "Epoch 2401/3000, Loss: 0.006233\n",
      "Epoch 2411/3000, Loss: 0.006190\n",
      "Epoch 2421/3000, Loss: 0.006147\n",
      "Epoch 2431/3000, Loss: 0.006104\n",
      "Epoch 2441/3000, Loss: 0.006062\n",
      "Epoch 2451/3000, Loss: 0.006021\n",
      "Epoch 2461/3000, Loss: 0.005979\n",
      "Epoch 2471/3000, Loss: 0.005938\n",
      "Epoch 2481/3000, Loss: 0.005898\n",
      "Epoch 2491/3000, Loss: 0.005857\n",
      "Epoch 2501/3000, Loss: 0.005817\n",
      "Epoch 2511/3000, Loss: 0.005778\n",
      "Epoch 2521/3000, Loss: 0.005738\n",
      "Epoch 2531/3000, Loss: 0.005699\n",
      "Epoch 2541/3000, Loss: 0.005661\n",
      "Epoch 2551/3000, Loss: 0.005623\n",
      "Epoch 2561/3000, Loss: 0.005584\n",
      "Epoch 2571/3000, Loss: 0.005546\n",
      "Epoch 2581/3000, Loss: 0.005509\n",
      "Epoch 2591/3000, Loss: 0.005472\n",
      "Epoch 2601/3000, Loss: 0.005435\n",
      "Epoch 2611/3000, Loss: 0.005399\n",
      "Epoch 2621/3000, Loss: 0.005362\n",
      "Epoch 2631/3000, Loss: 0.005326\n",
      "Epoch 2641/3000, Loss: 0.005291\n",
      "Epoch 2651/3000, Loss: 0.005255\n",
      "Epoch 2661/3000, Loss: 0.005220\n",
      "Epoch 2671/3000, Loss: 0.005185\n",
      "Epoch 2681/3000, Loss: 0.005151\n",
      "Epoch 2691/3000, Loss: 0.005116\n",
      "Epoch 2701/3000, Loss: 0.005082\n",
      "Epoch 2711/3000, Loss: 0.005048\n",
      "Epoch 2721/3000, Loss: 0.005015\n",
      "Epoch 2731/3000, Loss: 0.004982\n",
      "Epoch 2741/3000, Loss: 0.004949\n",
      "Epoch 2751/3000, Loss: 0.004916\n",
      "Epoch 2761/3000, Loss: 0.004883\n",
      "Epoch 2771/3000, Loss: 0.004851\n",
      "Epoch 2781/3000, Loss: 0.004819\n",
      "Epoch 2791/3000, Loss: 0.004787\n",
      "Epoch 2801/3000, Loss: 0.004756\n",
      "Epoch 2811/3000, Loss: 0.004724\n",
      "Epoch 2821/3000, Loss: 0.004693\n",
      "Epoch 2831/3000, Loss: 0.004662\n",
      "Epoch 2841/3000, Loss: 0.004631\n",
      "Epoch 2851/3000, Loss: 0.004601\n",
      "Epoch 2861/3000, Loss: 0.004571\n",
      "Epoch 2871/3000, Loss: 0.004541\n",
      "Epoch 2881/3000, Loss: 0.004511\n",
      "Epoch 2891/3000, Loss: 0.004481\n",
      "Epoch 2901/3000, Loss: 0.004452\n",
      "Epoch 2911/3000, Loss: 0.004423\n",
      "Epoch 2921/3000, Loss: 0.004394\n",
      "Epoch 2931/3000, Loss: 0.004365\n",
      "Epoch 2941/3000, Loss: 0.004337\n",
      "Epoch 2951/3000, Loss: 0.004309\n",
      "Epoch 2961/3000, Loss: 0.004280\n",
      "Epoch 2971/3000, Loss: 0.004252\n",
      "Epoch 2981/3000, Loss: 0.004224\n",
      "Epoch 2991/3000, Loss: 0.004197\n",
      "Fine-tuning with Soft-DTW\n",
      "Epoch 1/3000, Loss: 16.260010\n",
      "Converged at epoch 2\n",
      "\n",
      "--- EUCLIDEAN INITIALIZATION RESULTS ---\n",
      "softdtw_gamma1.0         : 6.0284\n",
      "softdtw_gamma0.1         : 5.9337\n",
      "softdtw_gamma0.01        : 5.6819\n",
      "softdtw_gamma0.001       : 6.2092\n"
     ]
    }
   ],
   "source": [
    "euclid_init_results = {}\n",
    "gammas = [1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "for gamma in gammas:\n",
    "    dtw_err = train_euclidean_init_then_softdtw(\n",
    "        gamma,\n",
    "        n_epochs_euclid=3000,\n",
    "        n_epochs_sdtw=3000\n",
    "    )\n",
    "    euclid_init_results[f\"softdtw_gamma{gamma}\"] = dtw_err\n",
    "\n",
    "print(\"\\n--- EUCLIDEAN INITIALIZATION RESULTS ---\")\n",
    "for k, v in euclid_init_results.items():\n",
    "    print(f\"{k:25s}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdb06555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'softdtw_gamma1.0': 6.028368127532158,\n",
       " 'softdtw_gamma0.1': 5.933686647854047,\n",
       " 'softdtw_gamma0.01': 5.681924049584777,\n",
       " 'softdtw_gamma0.001': 6.2091918509258175}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclid_init_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cae32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
